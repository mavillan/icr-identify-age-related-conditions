{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e38ec59a-8113-4290-bf8a-1637fb3ca5e7",
    "_uuid": "835f077d-5c8a-48e4-9a84-1bc2292eae88",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn import impute\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import (\n",
    "    plot_contour\n",
    "    , plot_edf\n",
    "    , plot_intermediate_values\n",
    "    , plot_optimization_history\n",
    "    , plot_parallel_coordinate\n",
    "    , plot_param_importances\n",
    "    , plot_slice\n",
    ")\n",
    "\n",
    "# custom modules\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "\n",
    "from preproc import load_data,scale_data\n",
    "#from calibration1 import calibrate_probs,optimize_calibration\n",
    "from calibration2 import calibrate_probs,optimize_calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_logloss_(y_pred, y_true, eps=1e-7):\n",
    "    n0 = np.sum(1-y_true)\n",
    "    n1 = np.sum(y_true)\n",
    "    p1 = np.clip(y_pred, eps, 1-eps)\n",
    "    p0 = 1-p1\n",
    "    log_loss0 = - np.sum((1-y_true) * np.log(p0)) / (n0+eps)\n",
    "    log_loss1 = - np.sum(y_true * np.log(p1)) / (n1+eps)\n",
    "    return (log_loss0 + log_loss1)/2\n",
    "\n",
    "\n",
    "def compute_overall_metric(oof_dfs:list) -> float:\n",
    "    all_metrics = [\n",
    "        balanced_logloss_(oof.pred_proba.values, oof.Class.values)\n",
    "        for oof in oof_dfs\n",
    "    ]\n",
    "    return np.mean(all_metrics)\n",
    "\n",
    "\n",
    "def calibrate_oof(oof_dfs, calib_params):\n",
    "    oof_dfs_calibrated = list()\n",
    "    \n",
    "    for oof in oof_dfs:\n",
    "        oof = oof.copy(deep=True)\n",
    "        calib_p1 = calibrate_probs(\n",
    "            oof.pred_proba.values,\n",
    "            **calib_params\n",
    "        )\n",
    "        oof[\"pred_proba\"] = calib_p1\n",
    "        oof_dfs_calibrated.append(oof)\n",
    "        \n",
    "    return oof_dfs_calibrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook parameter\n",
    "\n",
    "CALIBRATE_FIRST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# load data and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `lgbm-gbrt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_params_m01 = joblib.load(\"../data/iarc-lgbm-gbrt-bagging-balanced/calib_params.pkl\")\n",
    "oof_dfs_m01 = joblib.load(\"../data/iarc-lgbm-gbrt-bagging-balanced/oof_dataframes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_overall_metric(oof_dfs_m01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_dfs_calib_m01 = calibrate_oof(oof_dfs_m01, calib_params_m01)\n",
    "compute_overall_metric(oof_dfs_calib_m01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `lgbm-linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_params_m02 = joblib.load(\"../data/iarc-lgbm-linear-bagging-balanced/calib_params.pkl\")\n",
    "oof_dfs_m02 = joblib.load(\"../data/iarc-lgbm-linear-bagging-balanced/oof_dataframes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_overall_metric(oof_dfs_m02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_dfs_calib_m02 = calibrate_oof(oof_dfs_m02, calib_params_m02)\n",
    "compute_overall_metric(oof_dfs_calib_m02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `catboost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_params_m03 = joblib.load(\"../data/iarc-catboost-weight-balanced/calib_params.pkl\")\n",
    "oof_dfs_m03 = joblib.load(\"../data/iarc-catboost-weight-balanced/oof_dataframes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_overall_metric(oof_dfs_m03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_dfs_calib_m03 = calibrate_oof(oof_dfs_m03, calib_params_m03)\n",
    "compute_overall_metric(oof_dfs_calib_m03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. `tabpfn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_params_m04 = joblib.load(\"../data/iarc-tabpfn/calib_params.pkl\")\n",
    "oof_dfs_m04 = joblib.load(\"../data/iarc-tabpfn/oof_dataframes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_overall_metric(oof_dfs_m04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_dfs_calib_m04 = calibrate_oof(oof_dfs_m04, calib_params_m04)\n",
    "compute_overall_metric(oof_dfs_calib_m04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. `multiout-mlp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_params_m05 = joblib.load(\"../data/iarc-multiout-tf-mlp/calib_params.pkl\")\n",
    "oof_dfs_m05 = joblib.load(\"../data/iarc-multiout-tf-mlp/oof_dataframes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_overall_metric(oof_dfs_m05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_dfs_calib_m05 = calibrate_oof(oof_dfs_m05, calib_params_m05)\n",
    "compute_overall_metric(oof_dfs_calib_m05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. `xgb-gblinear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_params_m06 = joblib.load(\"../data/iarc-xgb-gblinear/calib_params.pkl\")\n",
    "oof_dfs_m06 = joblib.load(\"../data/iarc-xgb-gblinear/oof_dataframes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_overall_metric(oof_dfs_m06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_dfs_calib_m06 = calibrate_oof(oof_dfs_m06, calib_params_m06)\n",
    "compute_overall_metric(oof_dfs_calib_m06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. xgb-gbtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_params_m07 = joblib.load(\"../data/iarc-xgb-gbtree/calib_params.pkl\")\n",
    "oof_dfs_m07 = joblib.load(\"../data/iarc-xgb-gbtree/oof_dataframes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_overall_metric(oof_dfs_m07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_dfs_calib_m07 = calibrate_oof(oof_dfs_m07, calib_params_m07)\n",
    "compute_overall_metric(oof_dfs_calib_m07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## prepares data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models to consider\n",
    "\n",
    "if not CALIBRATE_FIRST:\n",
    "    print(\"using non calibrated\")\n",
    "    oof_all = [\n",
    "        oof_dfs_m01,\n",
    "        oof_dfs_m02,\n",
    "        oof_dfs_m03,\n",
    "        oof_dfs_m04,\n",
    "        oof_dfs_m05,\n",
    "        oof_dfs_m06,\n",
    "        oof_dfs_m07,\n",
    "    ]\n",
    "else:\n",
    "    print(\"using calibrated\")\n",
    "    oof_all = [\n",
    "        oof_dfs_calib_m01,\n",
    "        oof_dfs_calib_m02,\n",
    "        oof_dfs_calib_m03,\n",
    "        oof_dfs_calib_m04,\n",
    "        oof_dfs_calib_m05,\n",
    "        oof_dfs_calib_m06,\n",
    "        oof_dfs_calib_m07,\n",
    "    ]\n",
    "\n",
    "for _oof_dfs in oof_all:\n",
    "    print(compute_overall_metric(_oof_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../data/raw\"\n",
    "\n",
    "train = pd.read_csv(f\"{input_path}/train.csv\")\n",
    "test  = pd.read_csv(f\"{input_path}/test.csv\")\n",
    "greeks = pd.read_csv(f\"{input_path}/greeks.csv\")\n",
    "\n",
    "train.columns = [col.strip() for col in train.columns]\n",
    "test.columns = [col.strip() for col in test.columns]\n",
    "\n",
    "# available features\n",
    "input_cols = train.columns[1:-1]\n",
    "categ_cols = [\"EJ\"]\n",
    "\n",
    "# we extend train with dummies from greeks\n",
    "dummies = pd.get_dummies(greeks[[\"Alpha\",\"Beta\",\"Gamma\",\"Delta\"]])\n",
    "train[dummies.columns] = dummies\n",
    "\n",
    "# encode of categorical features\n",
    "encoder = preprocessing.LabelEncoder().fit(train[\"EJ\"])\n",
    "train[\"EJ\"] = encoder.transform(train[\"EJ\"]).astype(int)\n",
    "test[\"EJ\"] = encoder.transform(test[\"EJ\"]).astype(int)\n",
    "\n",
    "# impute missing values\n",
    "imputer = impute.SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(train[input_cols])\n",
    "train[input_cols] = imputer.transform(train[input_cols])\n",
    "test[input_cols] = imputer.transform(test[input_cols])\n",
    "\n",
    "# scale data\n",
    "scaler = preprocessing.MaxAbsScaler()\n",
    "scaler.fit(train[input_cols])\n",
    "train[input_cols] = scaler.transform(train[input_cols])\n",
    "test[input_cols] = scaler.transform(test[input_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_cv_split = joblib.load(\"../data/iarc-data-split/repeated_5fold_cv_split_4validation.pkl\")\n",
    "print(len(repeated_cv_split))\n",
    "\n",
    "# number of repetitions to use\n",
    "REPETITIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct = train.Class.value_counts(normalize=True)\n",
    "scale_pos_weight = pct[0]/pct[1]\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "cnt = train.Class.value_counts(normalize=False)\n",
    "neg_bagging_fraction = cnt[1]/cnt[0]\n",
    "print(\"neg_bagging_fraction:\", neg_bagging_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_oof_preds(train, input_cols, oof_all, repeat_nbr):\n",
    "    train = train.copy()\n",
    "    input_cols = input_cols.tolist().copy()\n",
    "    n_models = len(oof_all)\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        _oof = oof_all[i][repeat_nbr][[\"Id\",\"pred_proba\"]].rename({\"pred_proba\":f\"pm{i+1}\"}, axis=1)\n",
    "        train = pd.merge(train, _oof)\n",
    "        input_cols.append(f\"pm{i+1}\")\n",
    "        \n",
    "    return train,input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_logloss_(y_pred, y_true):\n",
    "    n0 = np.sum(1-y_true)\n",
    "    n1 = np.sum(y_true)\n",
    "    p1 = np.clip(y_pred, 1e-15, 1-1e-15)\n",
    "    p0 = 1-p1\n",
    "    log_loss0 = - np.sum((1-y_true) * np.log(p0)) / n0\n",
    "    log_loss1 = - np.sum(y_true * np.log(p1)) / n1\n",
    "    return (log_loss0 + log_loss1)/2\n",
    "\n",
    "def balanced_logloss(y_pred: np.ndarray, data: lgb.Dataset):\n",
    "    y_true = data.get_label()\n",
    "    return 'balanced_logloss', balanced_logloss_(y_pred, y_true), False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_method = \"bagging\"\n",
    "\n",
    "DEFAULT_PARAMS = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'None',\n",
    "    'learning_rate': 0.005,\n",
    "    'bagging_freq': 1,\n",
    "    'seed': 2112,\n",
    "    'first_metric_only': False,\n",
    "    'feature_pre_filter': False,\n",
    "    'verbosity': -1,\n",
    "    'linear_tree':True,\n",
    "    'n_jobs':8,\n",
    "}\n",
    "\n",
    "if balance_method == \"weight\":\n",
    "    DEFAULT_PARAMS[\"scale_pos_weight\"] = scale_pos_weight\n",
    "elif balance_method == \"bagging\":\n",
    "    DEFAULT_PARAMS[\"bagging_freq\"] = 1\n",
    "    DEFAULT_PARAMS[\"pos_bagging_fraction\"] = 1\n",
    "    DEFAULT_PARAMS[\"neg_bagging_fraction\"] = neg_bagging_fraction\n",
    "else:\n",
    "    print(\"Unknown balance_method\")\n",
    "    \n",
    "display(DEFAULT_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(\n",
    "        dataframe,\n",
    "        input_cols, \n",
    "        model_params,\n",
    "        repeated_cv_split,\n",
    "        n_repetitions=REPETITIONS,\n",
    "        verbose=False,\n",
    "    ):\n",
    "\n",
    "    metrics = list()\n",
    "    model_params = dict(model_params)\n",
    "    num_iterations = (\n",
    "        1000 if \"num_iterations\" not in model_params.keys() \n",
    "        else model_params.pop(\"num_iterations\")\n",
    "    )\n",
    "\n",
    "    for repeat in range(n_repetitions):\n",
    "        if verbose:\n",
    "            print(f\"REPEAT NUMBER: {repeat+1}/{n_repetitions}\")\n",
    "        cv_split = repeated_cv_split[f\"repeat_{repeat}\"]\n",
    "        n_folds = len(cv_split)\n",
    "        \n",
    "        for split in cv_split:\n",
    "            fold = split[\"fold\"]\n",
    "            train_idx = split[\"train_idx\"]\n",
    "            valid_idx = split[\"valid_idx\"]\n",
    "            if verbose:\n",
    "                print(f\"training model for fold: {fold+1}/{n_folds}\")\n",
    "\n",
    "            _train,_input_cols = include_oof_preds(train, input_cols, oof_all, repeat)\n",
    "\n",
    "            train_df = _train.loc[train_idx,:].reset_index(drop=True)\n",
    "            valid_df = _train.loc[valid_idx,:].reset_index(drop=True)\n",
    "\n",
    "            train_dset = lgb.Dataset(\n",
    "                data=train_df.loc[:,_input_cols],\n",
    "                label=train_df.loc[:,\"Class\"].values,\n",
    "                free_raw_data=False\n",
    "            )\n",
    "            model = lgb.train(\n",
    "                params=model_params,\n",
    "                train_set=train_dset,\n",
    "                num_boost_round=num_iterations,\n",
    "            )\n",
    "            \n",
    "            y_pred = model.predict(valid_df.loc[:,_input_cols])\n",
    "            metrics.append( balanced_logloss_(y_pred, valid_df.loc[:,\"Class\"].values) )\n",
    "    \n",
    "    return np.mean(metrics), np.std(metrics)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    sampled_params = dict(\n",
    "        # general booster config\n",
    "        max_bin = 2**trial.suggest_int(\"max_bin_exp\", 3, 8) - 1,\n",
    "        num_leaves = 2**trial.suggest_int(\"num_leaves_exp\", 2, 7) - 1,\n",
    "        num_iterations = trial.suggest_int(\"num_iterations\", 100, 2000),\n",
    "        # regularization\n",
    "        feature_fraction = trial.suggest_float(\"feature_fraction\", 0.2, 1.0, step=0.05),\n",
    "        min_data_in_leaf = trial.suggest_int(\"min_data_in_leaf\", 5, 100),\n",
    "        \n",
    "        lambda_l1 = trial.suggest_float(\"lambda_l1\", 1e-10, 1e2, log=True),\n",
    "        lambda_l2 = trial.suggest_float(\"lambda_l2\", 1e-10, 1e3, log=True),\n",
    "        path_smooth = trial.suggest_float(\"path_smooth\", 1e-10, 1e2, log=True),\n",
    "        min_gain_to_split = trial.suggest_float(\"min_gain_to_split\", 1e-10, 1e1, log=True),\n",
    "\n",
    "        # linear tree regularization parameter\n",
    "        linear_lambda = trial.suggest_float(\"linear_lambda\", 1e-10, 1e2, log=True),        \n",
    "    )\n",
    "    model_params = {**DEFAULT_PARAMS, **sampled_params}\n",
    "    \n",
    "    metric_mean, metric_std = train_validate(\n",
    "        dataframe = train,\n",
    "        input_cols = input_cols,\n",
    "        model_params = model_params,\n",
    "        repeated_cv_split = repeated_cv_split,\n",
    "        n_repetitions = REPETITIONS,\n",
    "        verbose = False,\n",
    "    )\n",
    "    \n",
    "    return metric_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_validate(\n",
    "    dataframe = train,\n",
    "    input_cols = input_cols,\n",
    "    model_params = DEFAULT_PARAMS,\n",
    "    repeated_cv_split = repeated_cv_split,\n",
    "    n_repetitions = REPETITIONS,\n",
    "    verbose = False\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_optimize = True\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"iarc-stacking-lgbm-linear-v2-try2\",\n",
    "    direction='minimize',\n",
    "    storage='sqlite:///iarc-stacking-lgbm-linear-v2-try2.db',\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "if do_optimize:\n",
    "    study.optimize(\n",
    "        objective, \n",
    "        n_trials=10_000, \n",
    "        timeout=21600, # 6 hours\n",
    "        n_jobs=1, \n",
    "        gc_after_trial=True,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.trials_dataframe().sort_values(\"value\", ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = dict(study.best_params)\n",
    "best_params[\"max_bin\"] = 2**best_params.pop(\"max_bin_exp\")-1\n",
    "best_params[\"num_leaves\"] = 2**best_params.pop(\"num_leaves_exp\")-1\n",
    "best_params = {**DEFAULT_PARAMS, **best_params}\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

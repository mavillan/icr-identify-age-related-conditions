{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "791e4da7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-15T16:40:04.138207Z",
     "iopub.status.busy": "2023-06-15T16:40:04.137567Z",
     "iopub.status.idle": "2023-06-15T16:40:06.933936Z",
     "shell.execute_reply": "2023-06-15T16:40:06.932672Z"
    },
    "papermill": {
     "duration": 2.806532,
     "end_time": "2023-06-15T16:40:06.936763",
     "exception": false,
     "start_time": "2023-06-15T16:40:04.130231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b684f",
   "metadata": {
    "papermill": {
     "duration": 0.004786,
     "end_time": "2023-06-15T16:40:06.947030",
     "exception": false,
     "start_time": "2023-06-15T16:40:06.942244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***\n",
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783b3296",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T16:40:06.959464Z",
     "iopub.status.busy": "2023-06-15T16:40:06.958758Z",
     "iopub.status.idle": "2023-06-15T16:40:07.142546Z",
     "shell.execute_reply": "2023-06-15T16:40:07.141381Z"
    },
    "papermill": {
     "duration": 0.19407,
     "end_time": "2023-06-15T16:40:07.146144",
     "exception": false,
     "start_time": "2023-06-15T16:40:06.952074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>BD</th>\n",
       "      <th>BN</th>\n",
       "      <th>BP</th>\n",
       "      <th>BQ</th>\n",
       "      <th>BR</th>\n",
       "      <th>BZ</th>\n",
       "      <th>CB</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>CF</th>\n",
       "      <th>CH</th>\n",
       "      <th>CL</th>\n",
       "      <th>CR</th>\n",
       "      <th>CS</th>\n",
       "      <th>CU</th>\n",
       "      <th>CW</th>\n",
       "      <th>DA</th>\n",
       "      <th>DE</th>\n",
       "      <th>DF</th>\n",
       "      <th>DH</th>\n",
       "      <th>DI</th>\n",
       "      <th>DL</th>\n",
       "      <th>DN</th>\n",
       "      <th>DU</th>\n",
       "      <th>DV</th>\n",
       "      <th>DY</th>\n",
       "      <th>EB</th>\n",
       "      <th>EE</th>\n",
       "      <th>EG</th>\n",
       "      <th>EH</th>\n",
       "      <th>EJ</th>\n",
       "      <th>EL</th>\n",
       "      <th>EP</th>\n",
       "      <th>EU</th>\n",
       "      <th>FC</th>\n",
       "      <th>FD</th>\n",
       "      <th>FE</th>\n",
       "      <th>FI</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GB</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GH</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "      <th>Class</th>\n",
       "      <th>Alpha_A</th>\n",
       "      <th>Alpha_B</th>\n",
       "      <th>Alpha_D</th>\n",
       "      <th>Alpha_G</th>\n",
       "      <th>Beta_A</th>\n",
       "      <th>Beta_B</th>\n",
       "      <th>Beta_C</th>\n",
       "      <th>Gamma_A</th>\n",
       "      <th>Gamma_B</th>\n",
       "      <th>Gamma_E</th>\n",
       "      <th>Gamma_F</th>\n",
       "      <th>Gamma_G</th>\n",
       "      <th>Gamma_H</th>\n",
       "      <th>Gamma_M</th>\n",
       "      <th>Gamma_N</th>\n",
       "      <th>Delta_A</th>\n",
       "      <th>Delta_B</th>\n",
       "      <th>Delta_C</th>\n",
       "      <th>Delta_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000ff2bfdfe9</td>\n",
       "      <td>0.209377</td>\n",
       "      <td>3109.03329</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>22.394407</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>9.812214</td>\n",
       "      <td>5.555634</td>\n",
       "      <td>4126.58731</td>\n",
       "      <td>22.5984</td>\n",
       "      <td>175.638726</td>\n",
       "      <td>152.707705</td>\n",
       "      <td>823.928241</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>47.223358</td>\n",
       "      <td>0.563481</td>\n",
       "      <td>23.387600</td>\n",
       "      <td>4.851915</td>\n",
       "      <td>0.023482</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.069225</td>\n",
       "      <td>13.784111</td>\n",
       "      <td>1.302012</td>\n",
       "      <td>36.205956</td>\n",
       "      <td>69.08340</td>\n",
       "      <td>295.570575</td>\n",
       "      <td>0.238680</td>\n",
       "      <td>0.284232</td>\n",
       "      <td>89.245560</td>\n",
       "      <td>84.31664</td>\n",
       "      <td>29.657104</td>\n",
       "      <td>5.310690</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>23.187704</td>\n",
       "      <td>7.294176</td>\n",
       "      <td>1.987283</td>\n",
       "      <td>1433.166750</td>\n",
       "      <td>0.949104</td>\n",
       "      <td>1</td>\n",
       "      <td>30.879420</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>3.828384</td>\n",
       "      <td>13.394640</td>\n",
       "      <td>10.265073</td>\n",
       "      <td>9028.291921</td>\n",
       "      <td>3.583450</td>\n",
       "      <td>7.298162</td>\n",
       "      <td>1.73855</td>\n",
       "      <td>0.094822</td>\n",
       "      <td>11.339138</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2003.810319</td>\n",
       "      <td>22.136229</td>\n",
       "      <td>69.834944</td>\n",
       "      <td>0.120343</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007255e47698</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>978.76416</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>36.968889</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.632190</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>13.517790</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5496.92824</td>\n",
       "      <td>19.4205</td>\n",
       "      <td>155.868030</td>\n",
       "      <td>14.754720</td>\n",
       "      <td>51.216883</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>30.284345</td>\n",
       "      <td>0.484710</td>\n",
       "      <td>50.628208</td>\n",
       "      <td>6.085041</td>\n",
       "      <td>0.031442</td>\n",
       "      <td>1.113875</td>\n",
       "      <td>1.117800</td>\n",
       "      <td>28.310953</td>\n",
       "      <td>1.357182</td>\n",
       "      <td>37.476568</td>\n",
       "      <td>70.79836</td>\n",
       "      <td>178.553100</td>\n",
       "      <td>0.238680</td>\n",
       "      <td>0.363489</td>\n",
       "      <td>110.581815</td>\n",
       "      <td>75.74548</td>\n",
       "      <td>37.532000</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>17.222328</td>\n",
       "      <td>4.926396</td>\n",
       "      <td>0.858603</td>\n",
       "      <td>1111.287150</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>95.415086</td>\n",
       "      <td>52.260480</td>\n",
       "      <td>17.175984</td>\n",
       "      <td>0.296850</td>\n",
       "      <td>6785.003474</td>\n",
       "      <td>10.358927</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>9.292698</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>27981.562750</td>\n",
       "      <td>29.135430</td>\n",
       "      <td>32.131996</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>013f2bd269f5</td>\n",
       "      <td>0.470030</td>\n",
       "      <td>2635.10654</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>32.360553</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>6.732840</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>12.824570</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5135.78024</td>\n",
       "      <td>26.4825</td>\n",
       "      <td>128.988531</td>\n",
       "      <td>219.320160</td>\n",
       "      <td>482.141594</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>32.563713</td>\n",
       "      <td>0.495852</td>\n",
       "      <td>85.955376</td>\n",
       "      <td>5.376488</td>\n",
       "      <td>0.036218</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.700350</td>\n",
       "      <td>39.364743</td>\n",
       "      <td>1.009611</td>\n",
       "      <td>21.459644</td>\n",
       "      <td>70.81970</td>\n",
       "      <td>321.426625</td>\n",
       "      <td>0.238680</td>\n",
       "      <td>0.210441</td>\n",
       "      <td>120.056438</td>\n",
       "      <td>65.46984</td>\n",
       "      <td>28.053464</td>\n",
       "      <td>1.289739</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>36.861352</td>\n",
       "      <td>7.813674</td>\n",
       "      <td>8.146651</td>\n",
       "      <td>1494.076488</td>\n",
       "      <td>0.377208</td>\n",
       "      <td>1</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>5.390628</td>\n",
       "      <td>224.207424</td>\n",
       "      <td>8.745201</td>\n",
       "      <td>8338.906181</td>\n",
       "      <td>11.626917</td>\n",
       "      <td>7.709560</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>1.198821</td>\n",
       "      <td>37.077772</td>\n",
       "      <td>88.609437</td>\n",
       "      <td>13676.957810</td>\n",
       "      <td>28.022851</td>\n",
       "      <td>35.192676</td>\n",
       "      <td>0.196941</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>043ac50845d5</td>\n",
       "      <td>0.252107</td>\n",
       "      <td>3819.65177</td>\n",
       "      <td>120.201618</td>\n",
       "      <td>77.112203</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.685344</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>11.053708</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>4169.67738</td>\n",
       "      <td>23.6577</td>\n",
       "      <td>237.282264</td>\n",
       "      <td>11.050410</td>\n",
       "      <td>661.518640</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>15.201914</td>\n",
       "      <td>0.717882</td>\n",
       "      <td>88.159360</td>\n",
       "      <td>2.347652</td>\n",
       "      <td>0.029054</td>\n",
       "      <td>1.400300</td>\n",
       "      <td>0.636075</td>\n",
       "      <td>41.116960</td>\n",
       "      <td>0.722727</td>\n",
       "      <td>21.530392</td>\n",
       "      <td>47.27586</td>\n",
       "      <td>196.607985</td>\n",
       "      <td>0.238680</td>\n",
       "      <td>0.292431</td>\n",
       "      <td>139.824570</td>\n",
       "      <td>71.57120</td>\n",
       "      <td>24.354856</td>\n",
       "      <td>2.655345</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>52.003884</td>\n",
       "      <td>7.386060</td>\n",
       "      <td>3.813326</td>\n",
       "      <td>15691.552180</td>\n",
       "      <td>0.614484</td>\n",
       "      <td>1</td>\n",
       "      <td>31.674357</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>31.323372</td>\n",
       "      <td>59.301984</td>\n",
       "      <td>7.884336</td>\n",
       "      <td>10965.766040</td>\n",
       "      <td>14.852022</td>\n",
       "      <td>6.122162</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>18.529584</td>\n",
       "      <td>82.416803</td>\n",
       "      <td>2094.262452</td>\n",
       "      <td>39.948656</td>\n",
       "      <td>90.493248</td>\n",
       "      <td>0.155829</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>044fb8a146ec</td>\n",
       "      <td>0.380297</td>\n",
       "      <td>3733.04844</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>14.103738</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.942255</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>102.151980</td>\n",
       "      <td>5728.73412</td>\n",
       "      <td>24.0108</td>\n",
       "      <td>324.546318</td>\n",
       "      <td>149.717165</td>\n",
       "      <td>6074.859475</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>82.213495</td>\n",
       "      <td>0.536467</td>\n",
       "      <td>72.644264</td>\n",
       "      <td>30.537722</td>\n",
       "      <td>0.025472</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.693150</td>\n",
       "      <td>31.724726</td>\n",
       "      <td>0.827550</td>\n",
       "      <td>34.415360</td>\n",
       "      <td>74.06532</td>\n",
       "      <td>200.178160</td>\n",
       "      <td>0.238680</td>\n",
       "      <td>0.207708</td>\n",
       "      <td>97.920120</td>\n",
       "      <td>52.83888</td>\n",
       "      <td>26.019912</td>\n",
       "      <td>1.144902</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>9.064856</td>\n",
       "      <td>7.350720</td>\n",
       "      <td>3.490846</td>\n",
       "      <td>1403.656300</td>\n",
       "      <td>0.164268</td>\n",
       "      <td>1</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>91.994825</td>\n",
       "      <td>51.141336</td>\n",
       "      <td>29.102640</td>\n",
       "      <td>4.274640</td>\n",
       "      <td>16198.049590</td>\n",
       "      <td>13.666727</td>\n",
       "      <td>8.153058</td>\n",
       "      <td>48.50134</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>16.408728</td>\n",
       "      <td>146.109943</td>\n",
       "      <td>8524.370502</td>\n",
       "      <td>45.381316</td>\n",
       "      <td>36.262628</td>\n",
       "      <td>0.096614</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>fd3dafe738fd</td>\n",
       "      <td>0.149555</td>\n",
       "      <td>3130.05946</td>\n",
       "      <td>123.763599</td>\n",
       "      <td>9.513984</td>\n",
       "      <td>13.020852</td>\n",
       "      <td>3.499305</td>\n",
       "      <td>0.077343</td>\n",
       "      <td>8.545512</td>\n",
       "      <td>2.804172</td>\n",
       "      <td>4157.68439</td>\n",
       "      <td>21.1860</td>\n",
       "      <td>167.877117</td>\n",
       "      <td>27.287375</td>\n",
       "      <td>365.516874</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>41.368691</td>\n",
       "      <td>0.691257</td>\n",
       "      <td>55.163024</td>\n",
       "      <td>4.780452</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>1.177525</td>\n",
       "      <td>0.698250</td>\n",
       "      <td>40.159779</td>\n",
       "      <td>1.070298</td>\n",
       "      <td>7.030640</td>\n",
       "      <td>21.75904</td>\n",
       "      <td>355.930925</td>\n",
       "      <td>0.238680</td>\n",
       "      <td>0.445479</td>\n",
       "      <td>176.977590</td>\n",
       "      <td>90.91832</td>\n",
       "      <td>27.957928</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>2.41906</td>\n",
       "      <td>32.508604</td>\n",
       "      <td>8.015112</td>\n",
       "      <td>1.354416</td>\n",
       "      <td>495.086300</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0</td>\n",
       "      <td>51.618996</td>\n",
       "      <td>78.526968</td>\n",
       "      <td>65.821872</td>\n",
       "      <td>29.708112</td>\n",
       "      <td>0.296850</td>\n",
       "      <td>17167.209610</td>\n",
       "      <td>9.879296</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>1.26092</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>8.967128</td>\n",
       "      <td>217.148554</td>\n",
       "      <td>8095.932828</td>\n",
       "      <td>24.640462</td>\n",
       "      <td>69.191944</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>fd895603f071</td>\n",
       "      <td>0.435846</td>\n",
       "      <td>5462.03438</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>46.551007</td>\n",
       "      <td>15.973224</td>\n",
       "      <td>5.979825</td>\n",
       "      <td>0.025882</td>\n",
       "      <td>12.622906</td>\n",
       "      <td>3.777550</td>\n",
       "      <td>5654.07556</td>\n",
       "      <td>27.1887</td>\n",
       "      <td>285.628059</td>\n",
       "      <td>344.644105</td>\n",
       "      <td>505.006814</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>61.910576</td>\n",
       "      <td>0.772304</td>\n",
       "      <td>85.233928</td>\n",
       "      <td>6.682597</td>\n",
       "      <td>0.038208</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.761025</td>\n",
       "      <td>39.852923</td>\n",
       "      <td>2.146113</td>\n",
       "      <td>33.648356</td>\n",
       "      <td>43.90996</td>\n",
       "      <td>157.393715</td>\n",
       "      <td>0.238680</td>\n",
       "      <td>0.437280</td>\n",
       "      <td>192.598575</td>\n",
       "      <td>123.17624</td>\n",
       "      <td>26.750080</td>\n",
       "      <td>0.648318</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>18.197092</td>\n",
       "      <td>8.976360</td>\n",
       "      <td>0.753797</td>\n",
       "      <td>1722.674025</td>\n",
       "      <td>0.139932</td>\n",
       "      <td>1</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>114.801199</td>\n",
       "      <td>447.657600</td>\n",
       "      <td>69.343680</td>\n",
       "      <td>6.067614</td>\n",
       "      <td>18460.330020</td>\n",
       "      <td>10.910227</td>\n",
       "      <td>10.223150</td>\n",
       "      <td>1.24236</td>\n",
       "      <td>0.426699</td>\n",
       "      <td>35.896418</td>\n",
       "      <td>496.994214</td>\n",
       "      <td>3085.308063</td>\n",
       "      <td>29.648928</td>\n",
       "      <td>124.808872</td>\n",
       "      <td>0.145340</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>fd8ef6377f76</td>\n",
       "      <td>0.427300</td>\n",
       "      <td>2459.10720</td>\n",
       "      <td>130.138587</td>\n",
       "      <td>55.355778</td>\n",
       "      <td>10.005552</td>\n",
       "      <td>8.070549</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>15.408390</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5888.87769</td>\n",
       "      <td>20.4798</td>\n",
       "      <td>178.661133</td>\n",
       "      <td>103.988995</td>\n",
       "      <td>2083.880500</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>90.411867</td>\n",
       "      <td>0.708616</td>\n",
       "      <td>142.680216</td>\n",
       "      <td>7.809288</td>\n",
       "      <td>0.027462</td>\n",
       "      <td>1.495775</td>\n",
       "      <td>0.879825</td>\n",
       "      <td>39.364743</td>\n",
       "      <td>1.489590</td>\n",
       "      <td>36.807176</td>\n",
       "      <td>104.62032</td>\n",
       "      <td>223.209115</td>\n",
       "      <td>0.238680</td>\n",
       "      <td>0.382620</td>\n",
       "      <td>218.915925</td>\n",
       "      <td>326.23620</td>\n",
       "      <td>26.463472</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>47.552312</td>\n",
       "      <td>9.478188</td>\n",
       "      <td>2.225112</td>\n",
       "      <td>2565.402825</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0</td>\n",
       "      <td>109.125159</td>\n",
       "      <td>87.397401</td>\n",
       "      <td>3.828384</td>\n",
       "      <td>71.725584</td>\n",
       "      <td>0.296850</td>\n",
       "      <td>5088.922912</td>\n",
       "      <td>12.029366</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>19.962092</td>\n",
       "      <td>128.896894</td>\n",
       "      <td>6474.652866</td>\n",
       "      <td>26.166072</td>\n",
       "      <td>119.559420</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>fe1942975e40</td>\n",
       "      <td>0.363205</td>\n",
       "      <td>1263.53524</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>23.685856</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>7.981959</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>7.524588</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>4517.86560</td>\n",
       "      <td>19.0674</td>\n",
       "      <td>119.162529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>722.377629</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>12.499760</td>\n",
       "      <td>0.602254</td>\n",
       "      <td>122.939496</td>\n",
       "      <td>2.964975</td>\n",
       "      <td>0.022288</td>\n",
       "      <td>1.050225</td>\n",
       "      <td>0.583125</td>\n",
       "      <td>34.367872</td>\n",
       "      <td>1.428903</td>\n",
       "      <td>36.699352</td>\n",
       "      <td>51.04140</td>\n",
       "      <td>112.196630</td>\n",
       "      <td>0.532818</td>\n",
       "      <td>0.549333</td>\n",
       "      <td>113.526045</td>\n",
       "      <td>96.97092</td>\n",
       "      <td>27.104928</td>\n",
       "      <td>0.510378</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>38.271840</td>\n",
       "      <td>10.078968</td>\n",
       "      <td>1.628524</td>\n",
       "      <td>1318.962875</td>\n",
       "      <td>0.139932</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.706633</td>\n",
       "      <td>8.259384</td>\n",
       "      <td>38.133312</td>\n",
       "      <td>6.192291</td>\n",
       "      <td>6464.250832</td>\n",
       "      <td>8.026928</td>\n",
       "      <td>9.256996</td>\n",
       "      <td>0.78764</td>\n",
       "      <td>0.670527</td>\n",
       "      <td>24.594488</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>1965.343176</td>\n",
       "      <td>25.116750</td>\n",
       "      <td>37.155112</td>\n",
       "      <td>0.184622</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>ffcca4ded3bb</td>\n",
       "      <td>0.482849</td>\n",
       "      <td>2672.53426</td>\n",
       "      <td>546.663930</td>\n",
       "      <td>112.006102</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.198099</td>\n",
       "      <td>0.116928</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>7.948668</td>\n",
       "      <td>2818.01707</td>\n",
       "      <td>21.1860</td>\n",
       "      <td>306.127863</td>\n",
       "      <td>6.090490</td>\n",
       "      <td>747.474930</td>\n",
       "      <td>257.432377</td>\n",
       "      <td>67.222974</td>\n",
       "      <td>0.644837</td>\n",
       "      <td>271.240664</td>\n",
       "      <td>10.479286</td>\n",
       "      <td>0.076018</td>\n",
       "      <td>1.241175</td>\n",
       "      <td>2.404275</td>\n",
       "      <td>42.799438</td>\n",
       "      <td>0.915822</td>\n",
       "      <td>37.824144</td>\n",
       "      <td>35.72704</td>\n",
       "      <td>889.496905</td>\n",
       "      <td>0.238680</td>\n",
       "      <td>0.172179</td>\n",
       "      <td>156.345390</td>\n",
       "      <td>82.54008</td>\n",
       "      <td>21.086160</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>1.74307</td>\n",
       "      <td>24.499368</td>\n",
       "      <td>7.873752</td>\n",
       "      <td>2.374259</td>\n",
       "      <td>912.311525</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0</td>\n",
       "      <td>15.960087</td>\n",
       "      <td>181.218219</td>\n",
       "      <td>78.370464</td>\n",
       "      <td>66.893232</td>\n",
       "      <td>0.296850</td>\n",
       "      <td>5895.352262</td>\n",
       "      <td>7.745765</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>1.14492</td>\n",
       "      <td>0.149006</td>\n",
       "      <td>13.673940</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>6850.484442</td>\n",
       "      <td>45.745974</td>\n",
       "      <td>114.842372</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id        AB          AF          AH          AM         AR  \\\n",
       "0    000ff2bfdfe9  0.209377  3109.03329   85.200147   22.394407   8.138688   \n",
       "1    007255e47698  0.145282   978.76416   85.200147   36.968889   8.138688   \n",
       "2    013f2bd269f5  0.470030  2635.10654   85.200147   32.360553   8.138688   \n",
       "3    043ac50845d5  0.252107  3819.65177  120.201618   77.112203   8.138688   \n",
       "4    044fb8a146ec  0.380297  3733.04844   85.200147   14.103738   8.138688   \n",
       "..            ...       ...         ...         ...         ...        ...   \n",
       "612  fd3dafe738fd  0.149555  3130.05946  123.763599    9.513984  13.020852   \n",
       "613  fd895603f071  0.435846  5462.03438   85.200147   46.551007  15.973224   \n",
       "614  fd8ef6377f76  0.427300  2459.10720  130.138587   55.355778  10.005552   \n",
       "615  fe1942975e40  0.363205  1263.53524   85.200147   23.685856   8.138688   \n",
       "616  ffcca4ded3bb  0.482849  2672.53426  546.663930  112.006102   8.138688   \n",
       "\n",
       "           AX        AY         AZ          BC          BD       BN  \\\n",
       "0    0.699861  0.025578   9.812214    5.555634  4126.58731  22.5984   \n",
       "1    3.632190  0.025578  13.517790    1.229900  5496.92824  19.4205   \n",
       "2    6.732840  0.025578  12.824570    1.229900  5135.78024  26.4825   \n",
       "3    3.685344  0.025578  11.053708    1.229900  4169.67738  23.6577   \n",
       "4    3.942255  0.054810   3.396778  102.151980  5728.73412  24.0108   \n",
       "..        ...       ...        ...         ...         ...      ...   \n",
       "612  3.499305  0.077343   8.545512    2.804172  4157.68439  21.1860   \n",
       "613  5.979825  0.025882  12.622906    3.777550  5654.07556  27.1887   \n",
       "614  8.070549  0.025578  15.408390    1.229900  5888.87769  20.4798   \n",
       "615  7.981959  0.025578   7.524588    1.229900  4517.86560  19.0674   \n",
       "616  3.198099  0.116928   3.396778    7.948668  2818.01707  21.1860   \n",
       "\n",
       "             BP          BQ           BR          BZ         CB        CC  \\\n",
       "0    175.638726  152.707705   823.928241  257.432377  47.223358  0.563481   \n",
       "1    155.868030   14.754720    51.216883  257.432377  30.284345  0.484710   \n",
       "2    128.988531  219.320160   482.141594  257.432377  32.563713  0.495852   \n",
       "3    237.282264   11.050410   661.518640  257.432377  15.201914  0.717882   \n",
       "4    324.546318  149.717165  6074.859475  257.432377  82.213495  0.536467   \n",
       "..          ...         ...          ...         ...        ...       ...   \n",
       "612  167.877117   27.287375   365.516874  257.432377  41.368691  0.691257   \n",
       "613  285.628059  344.644105   505.006814  257.432377  61.910576  0.772304   \n",
       "614  178.661133  103.988995  2083.880500  257.432377  90.411867  0.708616   \n",
       "615  119.162529         NaN   722.377629  257.432377  12.499760  0.602254   \n",
       "616  306.127863    6.090490   747.474930  257.432377  67.222974  0.644837   \n",
       "\n",
       "             CD         CF        CH        CL        CR         CS        CU  \\\n",
       "0     23.387600   4.851915  0.023482  1.050225  0.069225  13.784111  1.302012   \n",
       "1     50.628208   6.085041  0.031442  1.113875  1.117800  28.310953  1.357182   \n",
       "2     85.955376   5.376488  0.036218  1.050225  0.700350  39.364743  1.009611   \n",
       "3     88.159360   2.347652  0.029054  1.400300  0.636075  41.116960  0.722727   \n",
       "4     72.644264  30.537722  0.025472  1.050225  0.693150  31.724726  0.827550   \n",
       "..          ...        ...       ...       ...       ...        ...       ...   \n",
       "612   55.163024   4.780452  0.013930  1.177525  0.698250  40.159779  1.070298   \n",
       "613   85.233928   6.682597  0.038208  1.050225  0.761025  39.852923  2.146113   \n",
       "614  142.680216   7.809288  0.027462  1.495775  0.879825  39.364743  1.489590   \n",
       "615  122.939496   2.964975  0.022288  1.050225  0.583125  34.367872  1.428903   \n",
       "616  271.240664  10.479286  0.076018  1.241175  2.404275  42.799438  0.915822   \n",
       "\n",
       "            CW         DA          DE        DF        DH          DI  \\\n",
       "0    36.205956   69.08340  295.570575  0.238680  0.284232   89.245560   \n",
       "1    37.476568   70.79836  178.553100  0.238680  0.363489  110.581815   \n",
       "2    21.459644   70.81970  321.426625  0.238680  0.210441  120.056438   \n",
       "3    21.530392   47.27586  196.607985  0.238680  0.292431  139.824570   \n",
       "4    34.415360   74.06532  200.178160  0.238680  0.207708   97.920120   \n",
       "..         ...        ...         ...       ...       ...         ...   \n",
       "612   7.030640   21.75904  355.930925  0.238680  0.445479  176.977590   \n",
       "613  33.648356   43.90996  157.393715  0.238680  0.437280  192.598575   \n",
       "614  36.807176  104.62032  223.209115  0.238680  0.382620  218.915925   \n",
       "615  36.699352   51.04140  112.196630  0.532818  0.549333  113.526045   \n",
       "616  37.824144   35.72704  889.496905  0.238680  0.172179  156.345390   \n",
       "\n",
       "            DL         DN        DU       DV         DY         EB        EE  \\\n",
       "0     84.31664  29.657104  5.310690  1.74307  23.187704   7.294176  1.987283   \n",
       "1     75.74548  37.532000  0.005518  1.74307  17.222328   4.926396  0.858603   \n",
       "2     65.46984  28.053464  1.289739  1.74307  36.861352   7.813674  8.146651   \n",
       "3     71.57120  24.354856  2.655345  1.74307  52.003884   7.386060  3.813326   \n",
       "4     52.83888  26.019912  1.144902  1.74307   9.064856   7.350720  3.490846   \n",
       "..         ...        ...       ...      ...        ...        ...       ...   \n",
       "612   90.91832  27.957928  0.005518  2.41906  32.508604   8.015112  1.354416   \n",
       "613  123.17624  26.750080  0.648318  1.74307  18.197092   8.976360  0.753797   \n",
       "614  326.23620  26.463472  0.005518  1.74307  47.552312   9.478188  2.225112   \n",
       "615   96.97092  27.104928  0.510378  1.74307  38.271840  10.078968  1.628524   \n",
       "616   82.54008  21.086160  0.005518  1.74307  24.499368   7.873752  2.374259   \n",
       "\n",
       "               EG        EH  EJ          EL          EP          EU  \\\n",
       "0     1433.166750  0.949104   1   30.879420   78.526968    3.828384   \n",
       "1     1111.287150  0.003042   0  109.125159   95.415086   52.260480   \n",
       "2     1494.076488  0.377208   1  109.125159   78.526968    5.390628   \n",
       "3    15691.552180  0.614484   1   31.674357   78.526968   31.323372   \n",
       "4     1403.656300  0.164268   1  109.125159   91.994825   51.141336   \n",
       "..            ...       ...  ..         ...         ...         ...   \n",
       "612    495.086300  0.003042   0   51.618996   78.526968   65.821872   \n",
       "613   1722.674025  0.139932   1  109.125159  114.801199  447.657600   \n",
       "614   2565.402825  0.003042   0  109.125159   87.397401    3.828384   \n",
       "615   1318.962875  0.139932   1         NaN   99.706633    8.259384   \n",
       "616    912.311525  0.003042   0   15.960087  181.218219   78.370464   \n",
       "\n",
       "             FC         FD            FE         FI         FL        FR  \\\n",
       "0     13.394640  10.265073   9028.291921   3.583450   7.298162   1.73855   \n",
       "1     17.175984   0.296850   6785.003474  10.358927   0.173229   0.49706   \n",
       "2    224.207424   8.745201   8338.906181  11.626917   7.709560   0.97556   \n",
       "3     59.301984   7.884336  10965.766040  14.852022   6.122162   0.49706   \n",
       "4     29.102640   4.274640  16198.049590  13.666727   8.153058  48.50134   \n",
       "..          ...        ...           ...        ...        ...       ...   \n",
       "612   29.708112   0.296850  17167.209610   9.879296   0.173229   1.26092   \n",
       "613   69.343680   6.067614  18460.330020  10.910227  10.223150   1.24236   \n",
       "614   71.725584   0.296850   5088.922912  12.029366   0.173229   0.49706   \n",
       "615   38.133312   6.192291   6464.250832   8.026928   9.256996   0.78764   \n",
       "616   66.893232   0.296850   5895.352262   7.745765   0.173229   1.14492   \n",
       "\n",
       "           FS         GB          GE            GF         GH          GI  \\\n",
       "0    0.094822  11.339138   72.611063   2003.810319  22.136229   69.834944   \n",
       "1    0.568932   9.292698   72.611063  27981.562750  29.135430   32.131996   \n",
       "2    1.198821  37.077772   88.609437  13676.957810  28.022851   35.192676   \n",
       "3    0.284466  18.529584   82.416803   2094.262452  39.948656   90.493248   \n",
       "4    0.121914  16.408728  146.109943   8524.370502  45.381316   36.262628   \n",
       "..        ...        ...         ...           ...        ...         ...   \n",
       "612  0.067730   8.967128  217.148554   8095.932828  24.640462   69.191944   \n",
       "613  0.426699  35.896418  496.994214   3085.308063  29.648928  124.808872   \n",
       "614  0.067730  19.962092  128.896894   6474.652866  26.166072  119.559420   \n",
       "615  0.670527  24.594488   72.611063   1965.343176  25.116750   37.155112   \n",
       "616  0.149006  13.673940   72.611063   6850.484442  45.745974  114.842372   \n",
       "\n",
       "            GL  Class  Alpha_A  Alpha_B  Alpha_D  Alpha_G  Beta_A  Beta_B  \\\n",
       "0     0.120343      1        0        1        0        0       0       0   \n",
       "1    21.978000      0        1        0        0        0       0       0   \n",
       "2     0.196941      0        1        0        0        0       0       0   \n",
       "3     0.155829      0        1        0        0        0       0       0   \n",
       "4     0.096614      1        0        0        1        0       0       1   \n",
       "..         ...    ...      ...      ...      ...      ...     ...     ...   \n",
       "612  21.978000      0        1        0        0        0       0       1   \n",
       "613   0.145340      0        1        0        0        0       0       1   \n",
       "614  21.978000      0        1        0        0        0       0       0   \n",
       "615   0.184622      0        1        0        0        0       0       0   \n",
       "616  21.978000      0        1        0        0        0       0       0   \n",
       "\n",
       "     Beta_C  Gamma_A  Gamma_B  Gamma_E  Gamma_F  Gamma_G  Gamma_H  Gamma_M  \\\n",
       "0         1        0        0        0        0        1        0        0   \n",
       "1         1        0        0        0        0        0        0        1   \n",
       "2         1        0        0        0        0        0        0        1   \n",
       "3         1        0        0        0        0        0        0        1   \n",
       "4         0        0        0        0        1        0        0        0   \n",
       "..      ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "612       0        0        0        0        0        0        0        1   \n",
       "613       0        0        0        0        0        0        0        1   \n",
       "614       1        0        0        0        0        0        0        1   \n",
       "615       1        0        0        0        0        0        0        1   \n",
       "616       1        0        0        0        0        0        0        1   \n",
       "\n",
       "     Gamma_N  Delta_A  Delta_B  Delta_C  Delta_D  \n",
       "0          0        0        0        0        1  \n",
       "1          0        0        1        0        0  \n",
       "2          0        0        1        0        0  \n",
       "3          0        0        1        0        0  \n",
       "4          0        0        1        0        0  \n",
       "..       ...      ...      ...      ...      ...  \n",
       "612        0        0        1        0        0  \n",
       "613        0        0        1        0        0  \n",
       "614        0        0        1        0        0  \n",
       "615        0        0        1        0        0  \n",
       "616        0        0        1        0        0  \n",
       "\n",
       "[617 rows x 77 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/train.csv\")\n",
    "test  = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/test.csv\")\n",
    "greeks = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/greeks.csv\")\n",
    "\n",
    "train.columns = [col.strip() for col in train.columns]\n",
    "test.columns = [col.strip() for col in test.columns]\n",
    "\n",
    "# available features\n",
    "input_cols = train.columns[1:-1]\n",
    "categ_cols = [\"EJ\"]\n",
    "\n",
    "# we extend train with dummies from greeks\n",
    "dummies = pd.get_dummies(greeks[[\"Alpha\",\"Beta\",\"Gamma\",\"Delta\"]])\n",
    "train[dummies.columns] = dummies\n",
    "\n",
    "# encode of categorical features\n",
    "encoder = preprocessing.LabelEncoder().fit(train[\"EJ\"])\n",
    "train[\"EJ\"] = encoder.transform(train[\"EJ\"]).astype(int)\n",
    "test[\"EJ\"] = encoder.transform(test[\"EJ\"]).astype(int)\n",
    "\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5870a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T16:40:07.162299Z",
     "iopub.status.busy": "2023-06-15T16:40:07.161865Z",
     "iopub.status.idle": "2023-06-15T16:40:07.194880Z",
     "shell.execute_reply": "2023-06-15T16:40:07.193482Z"
    },
    "papermill": {
     "duration": 0.044654,
     "end_time": "2023-06-15T16:40:07.198018",
     "exception": false,
     "start_time": "2023-06-15T16:40:07.153364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "repeated_cv_split = joblib.load(\"/kaggle/input/iarc-data-split/repeated_5fold_cv_split_4validation.pkl\")\n",
    "print(len(repeated_cv_split))\n",
    "\n",
    "# number of repetitions to use\n",
    "REPETITIONS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa995e32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T16:05:50.036679Z",
     "iopub.status.busy": "2023-06-09T16:05:50.036272Z",
     "iopub.status.idle": "2023-06-09T16:05:50.061002Z",
     "shell.execute_reply": "2023-06-09T16:05:50.059984Z",
     "shell.execute_reply.started": "2023-06-09T16:05:50.036650Z"
    },
    "papermill": {
     "duration": 0.006725,
     "end_time": "2023-06-15T16:40:07.211800",
     "exception": false,
     "start_time": "2023-06-15T16:40:07.205075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***\n",
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3473651c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T16:40:07.227965Z",
     "iopub.status.busy": "2023-06-15T16:40:07.226739Z",
     "iopub.status.idle": "2023-06-15T16:40:07.234345Z",
     "shell.execute_reply": "2023-06-15T16:40:07.233496Z"
    },
    "papermill": {
     "duration": 0.018317,
     "end_time": "2023-06-15T16:40:07.236931",
     "exception": false,
     "start_time": "2023-06-15T16:40:07.218614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balanced_logloss(y_pred: np.ndarray, data: lgb.Dataset):\n",
    "    y_true = data.get_label()\n",
    "    n0 = np.sum(1-y_true)\n",
    "    n1 = np.sum(y_true)\n",
    "    p1 = np.clip(y_pred, 1e-15, 1-1e-15)\n",
    "    p0 = 1-p1\n",
    "    log_loss0 = - np.sum((1-y_true) * np.log(p0)) / n0\n",
    "    log_loss1 = - np.sum(y_true * np.log(p1)) / n1\n",
    "    return 'balanced_logloss', (log_loss0 + log_loss1)/2, False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1172b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T16:40:07.253061Z",
     "iopub.status.busy": "2023-06-15T16:40:07.252002Z",
     "iopub.status.idle": "2023-06-15T16:40:07.266524Z",
     "shell.execute_reply": "2023-06-15T16:40:07.265115Z"
    },
    "papermill": {
     "duration": 0.025022,
     "end_time": "2023-06-15T16:40:07.268825",
     "exception": false,
     "start_time": "2023-06-15T16:40:07.243803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight: 4.712962962962963\n"
     ]
    }
   ],
   "source": [
    "pct = train.Class.value_counts(normalize=True)\n",
    "pos_weight = pct[0]/pct[1]\n",
    "print(\"pos_weight:\", pos_weight)\n",
    "\n",
    "model_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'None',\n",
    "    'learning_rate': 0.005,\n",
    "    'num_leaves': 8,\n",
    "    'bagging_freq': 1,\n",
    "    'seed': 2112,\n",
    "    'verbosity': 0,\n",
    "    'first_metric_only': False,\n",
    "    'bin_construct_sample_cnt': int(1e6),\n",
    "    'feature_pre_filter': False,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'min_data_in_leaf': 10,\n",
    "    'path_smooth': 1,\n",
    "    'max_bin': 127,\n",
    "    # for dealing with unbalance\n",
    "    'scale_pos_weight':pos_weight,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f938737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T16:40:07.285383Z",
     "iopub.status.busy": "2023-06-15T16:40:07.284505Z",
     "iopub.status.idle": "2023-06-15T16:44:31.912792Z",
     "shell.execute_reply": "2023-06-15T16:44:31.911531Z"
    },
    "papermill": {
     "duration": 264.640328,
     "end_time": "2023-06-15T16:44:31.916375",
     "exception": false,
     "start_time": "2023-06-15T16:40:07.276047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPEAT NUMBER: 1/10\n",
      "training model for fold: 1/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.6621\n",
      "[200]\tvalid_0's balanced_logloss: 0.551111\n",
      "[300]\tvalid_0's balanced_logloss: 0.494658\n",
      "[400]\tvalid_0's balanced_logloss: 0.465152\n",
      "[500]\tvalid_0's balanced_logloss: 0.447034\n",
      "[600]\tvalid_0's balanced_logloss: 0.437699\n",
      "[700]\tvalid_0's balanced_logloss: 0.432168\n",
      "[800]\tvalid_0's balanced_logloss: 0.431416\n",
      "Early stopping, best iteration is:\n",
      "[763]\tvalid_0's balanced_logloss: 0.428656\n",
      "training model for fold: 2/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.612311\n",
      "[200]\tvalid_0's balanced_logloss: 0.476406\n",
      "[300]\tvalid_0's balanced_logloss: 0.407143\n",
      "[400]\tvalid_0's balanced_logloss: 0.362446\n",
      "[500]\tvalid_0's balanced_logloss: 0.334358\n",
      "[600]\tvalid_0's balanced_logloss: 0.313501\n",
      "[700]\tvalid_0's balanced_logloss: 0.299631\n",
      "[800]\tvalid_0's balanced_logloss: 0.289868\n",
      "[900]\tvalid_0's balanced_logloss: 0.283787\n",
      "[1000]\tvalid_0's balanced_logloss: 0.27795\n",
      "[1100]\tvalid_0's balanced_logloss: 0.274971\n",
      "[1200]\tvalid_0's balanced_logloss: 0.274235\n",
      "[1300]\tvalid_0's balanced_logloss: 0.27417\n",
      "Early stopping, best iteration is:\n",
      "[1248]\tvalid_0's balanced_logloss: 0.272704\n",
      "training model for fold: 3/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.578153\n",
      "[200]\tvalid_0's balanced_logloss: 0.438449\n",
      "[300]\tvalid_0's balanced_logloss: 0.357183\n",
      "[400]\tvalid_0's balanced_logloss: 0.313256\n",
      "[500]\tvalid_0's balanced_logloss: 0.284155\n",
      "[600]\tvalid_0's balanced_logloss: 0.261426\n",
      "[700]\tvalid_0's balanced_logloss: 0.248606\n",
      "[800]\tvalid_0's balanced_logloss: 0.235799\n",
      "[900]\tvalid_0's balanced_logloss: 0.226722\n",
      "[1000]\tvalid_0's balanced_logloss: 0.217873\n",
      "[1100]\tvalid_0's balanced_logloss: 0.213333\n",
      "[1200]\tvalid_0's balanced_logloss: 0.209748\n",
      "[1300]\tvalid_0's balanced_logloss: 0.20645\n",
      "[1400]\tvalid_0's balanced_logloss: 0.204572\n",
      "[1500]\tvalid_0's balanced_logloss: 0.202609\n",
      "[1600]\tvalid_0's balanced_logloss: 0.19887\n",
      "Early stopping, best iteration is:\n",
      "[1590]\tvalid_0's balanced_logloss: 0.198444\n",
      "training model for fold: 4/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.593746\n",
      "[200]\tvalid_0's balanced_logloss: 0.453883\n",
      "[300]\tvalid_0's balanced_logloss: 0.37792\n",
      "[400]\tvalid_0's balanced_logloss: 0.333043\n",
      "[500]\tvalid_0's balanced_logloss: 0.300288\n",
      "[600]\tvalid_0's balanced_logloss: 0.277241\n",
      "[700]\tvalid_0's balanced_logloss: 0.258612\n",
      "[800]\tvalid_0's balanced_logloss: 0.246386\n",
      "[900]\tvalid_0's balanced_logloss: 0.234972\n",
      "[1000]\tvalid_0's balanced_logloss: 0.223284\n",
      "[1100]\tvalid_0's balanced_logloss: 0.21507\n",
      "[1200]\tvalid_0's balanced_logloss: 0.20765\n",
      "[1300]\tvalid_0's balanced_logloss: 0.202018\n",
      "[1400]\tvalid_0's balanced_logloss: 0.197239\n",
      "[1500]\tvalid_0's balanced_logloss: 0.194902\n",
      "[1600]\tvalid_0's balanced_logloss: 0.191288\n",
      "[1700]\tvalid_0's balanced_logloss: 0.188686\n",
      "[1800]\tvalid_0's balanced_logloss: 0.183268\n",
      "[1900]\tvalid_0's balanced_logloss: 0.180163\n",
      "[2000]\tvalid_0's balanced_logloss: 0.175908\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1999]\tvalid_0's balanced_logloss: 0.175854\n",
      "training model for fold: 5/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.622199\n",
      "[200]\tvalid_0's balanced_logloss: 0.485515\n",
      "[300]\tvalid_0's balanced_logloss: 0.41596\n",
      "[400]\tvalid_0's balanced_logloss: 0.375998\n",
      "[500]\tvalid_0's balanced_logloss: 0.351752\n",
      "[600]\tvalid_0's balanced_logloss: 0.336673\n",
      "[700]\tvalid_0's balanced_logloss: 0.325793\n",
      "[800]\tvalid_0's balanced_logloss: 0.31914\n",
      "[900]\tvalid_0's balanced_logloss: 0.313189\n",
      "[1000]\tvalid_0's balanced_logloss: 0.310359\n",
      "[1100]\tvalid_0's balanced_logloss: 0.30926\n",
      "[1200]\tvalid_0's balanced_logloss: 0.308012\n",
      "[1300]\tvalid_0's balanced_logloss: 0.308421\n",
      "Early stopping, best iteration is:\n",
      "[1208]\tvalid_0's balanced_logloss: 0.307308\n",
      "REPEAT NUMBER: 2/10\n",
      "training model for fold: 1/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.621863\n",
      "[200]\tvalid_0's balanced_logloss: 0.50059\n",
      "[300]\tvalid_0's balanced_logloss: 0.437489\n",
      "[400]\tvalid_0's balanced_logloss: 0.396831\n",
      "[500]\tvalid_0's balanced_logloss: 0.368623\n",
      "[600]\tvalid_0's balanced_logloss: 0.34748\n",
      "[700]\tvalid_0's balanced_logloss: 0.335401\n",
      "[800]\tvalid_0's balanced_logloss: 0.328445\n",
      "[900]\tvalid_0's balanced_logloss: 0.32099\n",
      "[1000]\tvalid_0's balanced_logloss: 0.317283\n",
      "[1100]\tvalid_0's balanced_logloss: 0.312188\n",
      "[1200]\tvalid_0's balanced_logloss: 0.307575\n",
      "[1300]\tvalid_0's balanced_logloss: 0.304708\n",
      "[1400]\tvalid_0's balanced_logloss: 0.30613\n",
      "Early stopping, best iteration is:\n",
      "[1362]\tvalid_0's balanced_logloss: 0.303733\n",
      "training model for fold: 2/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000969 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.611693\n",
      "[200]\tvalid_0's balanced_logloss: 0.480151\n",
      "[300]\tvalid_0's balanced_logloss: 0.406723\n",
      "[400]\tvalid_0's balanced_logloss: 0.369173\n",
      "[500]\tvalid_0's balanced_logloss: 0.346859\n",
      "[600]\tvalid_0's balanced_logloss: 0.332697\n",
      "[700]\tvalid_0's balanced_logloss: 0.320931\n",
      "[800]\tvalid_0's balanced_logloss: 0.315971\n",
      "[900]\tvalid_0's balanced_logloss: 0.311876\n",
      "[1000]\tvalid_0's balanced_logloss: 0.309078\n",
      "[1100]\tvalid_0's balanced_logloss: 0.311089\n",
      "Early stopping, best iteration is:\n",
      "[1000]\tvalid_0's balanced_logloss: 0.309078\n",
      "training model for fold: 3/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.62108\n",
      "[200]\tvalid_0's balanced_logloss: 0.492851\n",
      "[300]\tvalid_0's balanced_logloss: 0.428177\n",
      "[400]\tvalid_0's balanced_logloss: 0.391879\n",
      "[500]\tvalid_0's balanced_logloss: 0.376561\n",
      "[600]\tvalid_0's balanced_logloss: 0.365753\n",
      "[700]\tvalid_0's balanced_logloss: 0.357488\n",
      "[800]\tvalid_0's balanced_logloss: 0.354257\n",
      "[900]\tvalid_0's balanced_logloss: 0.353243\n",
      "Early stopping, best iteration is:\n",
      "[858]\tvalid_0's balanced_logloss: 0.351987\n",
      "training model for fold: 4/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.592464\n",
      "[200]\tvalid_0's balanced_logloss: 0.447315\n",
      "[300]\tvalid_0's balanced_logloss: 0.365935\n",
      "[400]\tvalid_0's balanced_logloss: 0.316771\n",
      "[500]\tvalid_0's balanced_logloss: 0.282433\n",
      "[600]\tvalid_0's balanced_logloss: 0.255793\n",
      "[700]\tvalid_0's balanced_logloss: 0.241166\n",
      "[800]\tvalid_0's balanced_logloss: 0.228279\n",
      "[900]\tvalid_0's balanced_logloss: 0.214785\n",
      "[1000]\tvalid_0's balanced_logloss: 0.204368\n",
      "[1100]\tvalid_0's balanced_logloss: 0.197117\n",
      "[1200]\tvalid_0's balanced_logloss: 0.18852\n",
      "[1300]\tvalid_0's balanced_logloss: 0.182145\n",
      "[1400]\tvalid_0's balanced_logloss: 0.176819\n",
      "[1500]\tvalid_0's balanced_logloss: 0.173547\n",
      "[1600]\tvalid_0's balanced_logloss: 0.170342\n",
      "[1700]\tvalid_0's balanced_logloss: 0.167332\n",
      "[1800]\tvalid_0's balanced_logloss: 0.164591\n",
      "[1900]\tvalid_0's balanced_logloss: 0.163108\n",
      "[2000]\tvalid_0's balanced_logloss: 0.163146\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1936]\tvalid_0's balanced_logloss: 0.162523\n",
      "training model for fold: 5/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001681 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.604052\n",
      "[200]\tvalid_0's balanced_logloss: 0.470107\n",
      "[300]\tvalid_0's balanced_logloss: 0.404015\n",
      "[400]\tvalid_0's balanced_logloss: 0.369178\n",
      "[500]\tvalid_0's balanced_logloss: 0.350338\n",
      "[600]\tvalid_0's balanced_logloss: 0.337055\n",
      "[700]\tvalid_0's balanced_logloss: 0.329035\n",
      "[800]\tvalid_0's balanced_logloss: 0.323375\n",
      "[900]\tvalid_0's balanced_logloss: 0.31929\n",
      "[1000]\tvalid_0's balanced_logloss: 0.316895\n",
      "[1100]\tvalid_0's balanced_logloss: 0.314213\n",
      "[1200]\tvalid_0's balanced_logloss: 0.315042\n",
      "Early stopping, best iteration is:\n",
      "[1113]\tvalid_0's balanced_logloss: 0.313749\n",
      "REPEAT NUMBER: 3/10\n",
      "training model for fold: 1/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.610731\n",
      "[200]\tvalid_0's balanced_logloss: 0.493032\n",
      "[300]\tvalid_0's balanced_logloss: 0.43245\n",
      "[400]\tvalid_0's balanced_logloss: 0.399726\n",
      "[500]\tvalid_0's balanced_logloss: 0.375101\n",
      "[600]\tvalid_0's balanced_logloss: 0.355732\n",
      "[700]\tvalid_0's balanced_logloss: 0.342457\n",
      "[800]\tvalid_0's balanced_logloss: 0.333207\n",
      "[900]\tvalid_0's balanced_logloss: 0.325993\n",
      "[1000]\tvalid_0's balanced_logloss: 0.318634\n",
      "[1100]\tvalid_0's balanced_logloss: 0.312801\n",
      "[1200]\tvalid_0's balanced_logloss: 0.306945\n",
      "[1300]\tvalid_0's balanced_logloss: 0.301205\n",
      "[1400]\tvalid_0's balanced_logloss: 0.300715\n",
      "[1500]\tvalid_0's balanced_logloss: 0.296645\n",
      "[1600]\tvalid_0's balanced_logloss: 0.29326\n",
      "[1700]\tvalid_0's balanced_logloss: 0.289036\n",
      "[1800]\tvalid_0's balanced_logloss: 0.288282\n",
      "Early stopping, best iteration is:\n",
      "[1765]\tvalid_0's balanced_logloss: 0.287489\n",
      "training model for fold: 2/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.631527\n",
      "[200]\tvalid_0's balanced_logloss: 0.506122\n",
      "[300]\tvalid_0's balanced_logloss: 0.435218\n",
      "[400]\tvalid_0's balanced_logloss: 0.394209\n",
      "[500]\tvalid_0's balanced_logloss: 0.367749\n",
      "[600]\tvalid_0's balanced_logloss: 0.351146\n",
      "[700]\tvalid_0's balanced_logloss: 0.34013\n",
      "[800]\tvalid_0's balanced_logloss: 0.330188\n",
      "[900]\tvalid_0's balanced_logloss: 0.321196\n",
      "[1000]\tvalid_0's balanced_logloss: 0.316677\n",
      "[1100]\tvalid_0's balanced_logloss: 0.314609\n",
      "Early stopping, best iteration is:\n",
      "[1063]\tvalid_0's balanced_logloss: 0.313698\n",
      "training model for fold: 3/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.621156\n",
      "[200]\tvalid_0's balanced_logloss: 0.476384\n",
      "[300]\tvalid_0's balanced_logloss: 0.398243\n",
      "[400]\tvalid_0's balanced_logloss: 0.348778\n",
      "[500]\tvalid_0's balanced_logloss: 0.313736\n",
      "[600]\tvalid_0's balanced_logloss: 0.287656\n",
      "[700]\tvalid_0's balanced_logloss: 0.269226\n",
      "[800]\tvalid_0's balanced_logloss: 0.2551\n",
      "[900]\tvalid_0's balanced_logloss: 0.244387\n",
      "[1000]\tvalid_0's balanced_logloss: 0.236972\n",
      "[1100]\tvalid_0's balanced_logloss: 0.231759\n",
      "[1200]\tvalid_0's balanced_logloss: 0.22686\n",
      "[1300]\tvalid_0's balanced_logloss: 0.224108\n",
      "[1400]\tvalid_0's balanced_logloss: 0.222769\n",
      "[1500]\tvalid_0's balanced_logloss: 0.221353\n",
      "[1600]\tvalid_0's balanced_logloss: 0.221671\n",
      "Early stopping, best iteration is:\n",
      "[1529]\tvalid_0's balanced_logloss: 0.220961\n",
      "training model for fold: 4/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.584685\n",
      "[200]\tvalid_0's balanced_logloss: 0.436828\n",
      "[300]\tvalid_0's balanced_logloss: 0.358385\n",
      "[400]\tvalid_0's balanced_logloss: 0.311346\n",
      "[500]\tvalid_0's balanced_logloss: 0.283652\n",
      "[600]\tvalid_0's balanced_logloss: 0.260287\n",
      "[700]\tvalid_0's balanced_logloss: 0.243898\n",
      "[800]\tvalid_0's balanced_logloss: 0.231542\n",
      "[900]\tvalid_0's balanced_logloss: 0.223179\n",
      "[1000]\tvalid_0's balanced_logloss: 0.215996\n",
      "[1100]\tvalid_0's balanced_logloss: 0.211346\n",
      "[1200]\tvalid_0's balanced_logloss: 0.20755\n",
      "[1300]\tvalid_0's balanced_logloss: 0.20628\n",
      "[1400]\tvalid_0's balanced_logloss: 0.203425\n",
      "[1500]\tvalid_0's balanced_logloss: 0.201411\n",
      "[1600]\tvalid_0's balanced_logloss: 0.200875\n",
      "[1700]\tvalid_0's balanced_logloss: 0.201141\n",
      "Early stopping, best iteration is:\n",
      "[1611]\tvalid_0's balanced_logloss: 0.200485\n",
      "training model for fold: 5/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.598148\n",
      "[200]\tvalid_0's balanced_logloss: 0.472906\n",
      "[300]\tvalid_0's balanced_logloss: 0.409676\n",
      "[400]\tvalid_0's balanced_logloss: 0.373716\n",
      "[500]\tvalid_0's balanced_logloss: 0.353405\n",
      "[600]\tvalid_0's balanced_logloss: 0.340296\n",
      "[700]\tvalid_0's balanced_logloss: 0.327391\n",
      "[800]\tvalid_0's balanced_logloss: 0.318654\n",
      "[900]\tvalid_0's balanced_logloss: 0.31171\n",
      "[1000]\tvalid_0's balanced_logloss: 0.305502\n",
      "[1100]\tvalid_0's balanced_logloss: 0.302436\n",
      "[1200]\tvalid_0's balanced_logloss: 0.301101\n",
      "[1300]\tvalid_0's balanced_logloss: 0.302545\n",
      "Early stopping, best iteration is:\n",
      "[1203]\tvalid_0's balanced_logloss: 0.301011\n",
      "REPEAT NUMBER: 4/10\n",
      "training model for fold: 1/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.60473\n",
      "[200]\tvalid_0's balanced_logloss: 0.468079\n",
      "[300]\tvalid_0's balanced_logloss: 0.387226\n",
      "[400]\tvalid_0's balanced_logloss: 0.342093\n",
      "[500]\tvalid_0's balanced_logloss: 0.307132\n",
      "[600]\tvalid_0's balanced_logloss: 0.281459\n",
      "[700]\tvalid_0's balanced_logloss: 0.264595\n",
      "[800]\tvalid_0's balanced_logloss: 0.24845\n",
      "[900]\tvalid_0's balanced_logloss: 0.238081\n",
      "[1000]\tvalid_0's balanced_logloss: 0.229823\n",
      "[1100]\tvalid_0's balanced_logloss: 0.223884\n",
      "[1200]\tvalid_0's balanced_logloss: 0.216527\n",
      "[1300]\tvalid_0's balanced_logloss: 0.212226\n",
      "[1400]\tvalid_0's balanced_logloss: 0.206594\n",
      "[1500]\tvalid_0's balanced_logloss: 0.202549\n",
      "[1600]\tvalid_0's balanced_logloss: 0.200356\n",
      "[1700]\tvalid_0's balanced_logloss: 0.197269\n",
      "[1800]\tvalid_0's balanced_logloss: 0.195693\n",
      "[1900]\tvalid_0's balanced_logloss: 0.196299\n",
      "Early stopping, best iteration is:\n",
      "[1810]\tvalid_0's balanced_logloss: 0.195318\n",
      "training model for fold: 2/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.646149\n",
      "[200]\tvalid_0's balanced_logloss: 0.517624\n",
      "[300]\tvalid_0's balanced_logloss: 0.452272\n",
      "[400]\tvalid_0's balanced_logloss: 0.414227\n",
      "[500]\tvalid_0's balanced_logloss: 0.39194\n",
      "[600]\tvalid_0's balanced_logloss: 0.374474\n",
      "[700]\tvalid_0's balanced_logloss: 0.367865\n",
      "[800]\tvalid_0's balanced_logloss: 0.360303\n",
      "[900]\tvalid_0's balanced_logloss: 0.358308\n",
      "[1000]\tvalid_0's balanced_logloss: 0.356845\n",
      "Early stopping, best iteration is:\n",
      "[983]\tvalid_0's balanced_logloss: 0.355748\n",
      "training model for fold: 3/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001217 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.616842\n",
      "[200]\tvalid_0's balanced_logloss: 0.494427\n",
      "[300]\tvalid_0's balanced_logloss: 0.435347\n",
      "[400]\tvalid_0's balanced_logloss: 0.400711\n",
      "[500]\tvalid_0's balanced_logloss: 0.383609\n",
      "[600]\tvalid_0's balanced_logloss: 0.368864\n",
      "[700]\tvalid_0's balanced_logloss: 0.35845\n",
      "[800]\tvalid_0's balanced_logloss: 0.353112\n",
      "[900]\tvalid_0's balanced_logloss: 0.351284\n",
      "[1000]\tvalid_0's balanced_logloss: 0.34804\n",
      "Early stopping, best iteration is:\n",
      "[994]\tvalid_0's balanced_logloss: 0.347796\n",
      "training model for fold: 4/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.565068\n",
      "[200]\tvalid_0's balanced_logloss: 0.41355\n",
      "[300]\tvalid_0's balanced_logloss: 0.337368\n",
      "[400]\tvalid_0's balanced_logloss: 0.290207\n",
      "[500]\tvalid_0's balanced_logloss: 0.259392\n",
      "[600]\tvalid_0's balanced_logloss: 0.23843\n",
      "[700]\tvalid_0's balanced_logloss: 0.223861\n",
      "[800]\tvalid_0's balanced_logloss: 0.215771\n",
      "[900]\tvalid_0's balanced_logloss: 0.208064\n",
      "[1000]\tvalid_0's balanced_logloss: 0.204189\n",
      "[1100]\tvalid_0's balanced_logloss: 0.200278\n",
      "[1200]\tvalid_0's balanced_logloss: 0.198083\n",
      "[1300]\tvalid_0's balanced_logloss: 0.193934\n",
      "[1400]\tvalid_0's balanced_logloss: 0.191864\n",
      "[1500]\tvalid_0's balanced_logloss: 0.191408\n",
      "[1600]\tvalid_0's balanced_logloss: 0.187905\n",
      "[1700]\tvalid_0's balanced_logloss: 0.187063\n",
      "[1800]\tvalid_0's balanced_logloss: 0.186422\n",
      "Early stopping, best iteration is:\n",
      "[1778]\tvalid_0's balanced_logloss: 0.18576\n",
      "training model for fold: 5/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001282 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.631035\n",
      "[200]\tvalid_0's balanced_logloss: 0.502101\n",
      "[300]\tvalid_0's balanced_logloss: 0.427088\n",
      "[400]\tvalid_0's balanced_logloss: 0.385582\n",
      "[500]\tvalid_0's balanced_logloss: 0.356907\n",
      "[600]\tvalid_0's balanced_logloss: 0.33967\n",
      "[700]\tvalid_0's balanced_logloss: 0.328211\n",
      "[800]\tvalid_0's balanced_logloss: 0.315593\n",
      "[900]\tvalid_0's balanced_logloss: 0.306932\n",
      "[1000]\tvalid_0's balanced_logloss: 0.302089\n",
      "[1100]\tvalid_0's balanced_logloss: 0.296347\n",
      "[1200]\tvalid_0's balanced_logloss: 0.29213\n",
      "[1300]\tvalid_0's balanced_logloss: 0.288037\n",
      "[1400]\tvalid_0's balanced_logloss: 0.285413\n",
      "[1500]\tvalid_0's balanced_logloss: 0.283461\n",
      "[1600]\tvalid_0's balanced_logloss: 0.282924\n",
      "[1700]\tvalid_0's balanced_logloss: 0.282437\n",
      "[1800]\tvalid_0's balanced_logloss: 0.281518\n",
      "[1900]\tvalid_0's balanced_logloss: 0.279192\n",
      "[2000]\tvalid_0's balanced_logloss: 0.278354\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1991]\tvalid_0's balanced_logloss: 0.277994\n",
      "REPEAT NUMBER: 5/10\n",
      "training model for fold: 1/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.620321\n",
      "[200]\tvalid_0's balanced_logloss: 0.489735\n",
      "[300]\tvalid_0's balanced_logloss: 0.419515\n",
      "[400]\tvalid_0's balanced_logloss: 0.375304\n",
      "[500]\tvalid_0's balanced_logloss: 0.349052\n",
      "[600]\tvalid_0's balanced_logloss: 0.328184\n",
      "[700]\tvalid_0's balanced_logloss: 0.31502\n",
      "[800]\tvalid_0's balanced_logloss: 0.302572\n",
      "[900]\tvalid_0's balanced_logloss: 0.295654\n",
      "[1000]\tvalid_0's balanced_logloss: 0.289374\n",
      "[1100]\tvalid_0's balanced_logloss: 0.283075\n",
      "[1200]\tvalid_0's balanced_logloss: 0.279635\n",
      "[1300]\tvalid_0's balanced_logloss: 0.276464\n",
      "[1400]\tvalid_0's balanced_logloss: 0.275544\n",
      "[1500]\tvalid_0's balanced_logloss: 0.273673\n",
      "[1600]\tvalid_0's balanced_logloss: 0.270868\n",
      "[1700]\tvalid_0's balanced_logloss: 0.269059\n",
      "Early stopping, best iteration is:\n",
      "[1688]\tvalid_0's balanced_logloss: 0.268408\n",
      "training model for fold: 2/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.578014\n",
      "[200]\tvalid_0's balanced_logloss: 0.440075\n",
      "[300]\tvalid_0's balanced_logloss: 0.368085\n",
      "[400]\tvalid_0's balanced_logloss: 0.323772\n",
      "[500]\tvalid_0's balanced_logloss: 0.29599\n",
      "[600]\tvalid_0's balanced_logloss: 0.274407\n",
      "[700]\tvalid_0's balanced_logloss: 0.260656\n",
      "[800]\tvalid_0's balanced_logloss: 0.249818\n",
      "[900]\tvalid_0's balanced_logloss: 0.238508\n",
      "[1000]\tvalid_0's balanced_logloss: 0.230487\n",
      "[1100]\tvalid_0's balanced_logloss: 0.223412\n",
      "[1200]\tvalid_0's balanced_logloss: 0.214896\n",
      "[1300]\tvalid_0's balanced_logloss: 0.211184\n",
      "[1400]\tvalid_0's balanced_logloss: 0.206402\n",
      "[1500]\tvalid_0's balanced_logloss: 0.202673\n",
      "[1600]\tvalid_0's balanced_logloss: 0.201825\n",
      "[1700]\tvalid_0's balanced_logloss: 0.199461\n",
      "[1800]\tvalid_0's balanced_logloss: 0.195925\n",
      "[1900]\tvalid_0's balanced_logloss: 0.191732\n",
      "[2000]\tvalid_0's balanced_logloss: 0.188839\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's balanced_logloss: 0.188839\n",
      "training model for fold: 3/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.617709\n",
      "[200]\tvalid_0's balanced_logloss: 0.48732\n",
      "[300]\tvalid_0's balanced_logloss: 0.424363\n",
      "[400]\tvalid_0's balanced_logloss: 0.385262\n",
      "[500]\tvalid_0's balanced_logloss: 0.355914\n",
      "[600]\tvalid_0's balanced_logloss: 0.333131\n",
      "[700]\tvalid_0's balanced_logloss: 0.318496\n",
      "[800]\tvalid_0's balanced_logloss: 0.307697\n",
      "[900]\tvalid_0's balanced_logloss: 0.297199\n",
      "[1000]\tvalid_0's balanced_logloss: 0.291901\n",
      "[1100]\tvalid_0's balanced_logloss: 0.288973\n",
      "[1200]\tvalid_0's balanced_logloss: 0.281663\n",
      "[1300]\tvalid_0's balanced_logloss: 0.278092\n",
      "[1400]\tvalid_0's balanced_logloss: 0.276758\n",
      "[1500]\tvalid_0's balanced_logloss: 0.276133\n",
      "[1600]\tvalid_0's balanced_logloss: 0.275758\n",
      "Early stopping, best iteration is:\n",
      "[1557]\tvalid_0's balanced_logloss: 0.274494\n",
      "training model for fold: 4/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.581658\n",
      "[200]\tvalid_0's balanced_logloss: 0.432075\n",
      "[300]\tvalid_0's balanced_logloss: 0.349686\n",
      "[400]\tvalid_0's balanced_logloss: 0.300807\n",
      "[500]\tvalid_0's balanced_logloss: 0.266404\n",
      "[600]\tvalid_0's balanced_logloss: 0.240482\n",
      "[700]\tvalid_0's balanced_logloss: 0.220833\n",
      "[800]\tvalid_0's balanced_logloss: 0.205941\n",
      "[900]\tvalid_0's balanced_logloss: 0.192508\n",
      "[1000]\tvalid_0's balanced_logloss: 0.183387\n",
      "[1100]\tvalid_0's balanced_logloss: 0.175024\n",
      "[1200]\tvalid_0's balanced_logloss: 0.167473\n",
      "[1300]\tvalid_0's balanced_logloss: 0.162206\n",
      "[1400]\tvalid_0's balanced_logloss: 0.15581\n",
      "[1500]\tvalid_0's balanced_logloss: 0.151529\n",
      "[1600]\tvalid_0's balanced_logloss: 0.146332\n",
      "[1700]\tvalid_0's balanced_logloss: 0.142774\n",
      "[1800]\tvalid_0's balanced_logloss: 0.138857\n",
      "[1900]\tvalid_0's balanced_logloss: 0.135988\n",
      "[2000]\tvalid_0's balanced_logloss: 0.134584\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's balanced_logloss: 0.134584\n",
      "training model for fold: 5/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.656903\n",
      "[200]\tvalid_0's balanced_logloss: 0.546157\n",
      "[300]\tvalid_0's balanced_logloss: 0.492793\n",
      "[400]\tvalid_0's balanced_logloss: 0.465483\n",
      "[500]\tvalid_0's balanced_logloss: 0.459759\n",
      "[600]\tvalid_0's balanced_logloss: 0.455214\n",
      "[700]\tvalid_0's balanced_logloss: 0.456041\n",
      "Early stopping, best iteration is:\n",
      "[672]\tvalid_0's balanced_logloss: 0.45341\n",
      "REPEAT NUMBER: 6/10\n",
      "training model for fold: 1/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.605946\n",
      "[200]\tvalid_0's balanced_logloss: 0.465393\n",
      "[300]\tvalid_0's balanced_logloss: 0.394146\n",
      "[400]\tvalid_0's balanced_logloss: 0.352771\n",
      "[500]\tvalid_0's balanced_logloss: 0.320998\n",
      "[600]\tvalid_0's balanced_logloss: 0.29973\n",
      "[700]\tvalid_0's balanced_logloss: 0.281664\n",
      "[800]\tvalid_0's balanced_logloss: 0.270721\n",
      "[900]\tvalid_0's balanced_logloss: 0.259118\n",
      "[1000]\tvalid_0's balanced_logloss: 0.246439\n",
      "[1100]\tvalid_0's balanced_logloss: 0.23632\n",
      "[1200]\tvalid_0's balanced_logloss: 0.229101\n",
      "[1300]\tvalid_0's balanced_logloss: 0.223265\n",
      "[1400]\tvalid_0's balanced_logloss: 0.217223\n",
      "[1500]\tvalid_0's balanced_logloss: 0.210132\n",
      "[1600]\tvalid_0's balanced_logloss: 0.205325\n",
      "[1700]\tvalid_0's balanced_logloss: 0.200967\n",
      "[1800]\tvalid_0's balanced_logloss: 0.195977\n",
      "[1900]\tvalid_0's balanced_logloss: 0.193717\n",
      "[2000]\tvalid_0's balanced_logloss: 0.190368\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's balanced_logloss: 0.190368\n",
      "training model for fold: 2/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.606468\n",
      "[200]\tvalid_0's balanced_logloss: 0.471508\n",
      "[300]\tvalid_0's balanced_logloss: 0.405018\n",
      "[400]\tvalid_0's balanced_logloss: 0.362806\n",
      "[500]\tvalid_0's balanced_logloss: 0.334652\n",
      "[600]\tvalid_0's balanced_logloss: 0.315548\n",
      "[700]\tvalid_0's balanced_logloss: 0.300234\n",
      "[800]\tvalid_0's balanced_logloss: 0.29217\n",
      "[900]\tvalid_0's balanced_logloss: 0.286172\n",
      "[1000]\tvalid_0's balanced_logloss: 0.28321\n",
      "[1100]\tvalid_0's balanced_logloss: 0.280415\n",
      "[1200]\tvalid_0's balanced_logloss: 0.278381\n",
      "[1300]\tvalid_0's balanced_logloss: 0.276424\n",
      "[1400]\tvalid_0's balanced_logloss: 0.277902\n",
      "Early stopping, best iteration is:\n",
      "[1341]\tvalid_0's balanced_logloss: 0.274874\n",
      "training model for fold: 3/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.619518\n",
      "[200]\tvalid_0's balanced_logloss: 0.495437\n",
      "[300]\tvalid_0's balanced_logloss: 0.428713\n",
      "[400]\tvalid_0's balanced_logloss: 0.388405\n",
      "[500]\tvalid_0's balanced_logloss: 0.358425\n",
      "[600]\tvalid_0's balanced_logloss: 0.340126\n",
      "[700]\tvalid_0's balanced_logloss: 0.327751\n",
      "[800]\tvalid_0's balanced_logloss: 0.317064\n",
      "[900]\tvalid_0's balanced_logloss: 0.31093\n",
      "[1000]\tvalid_0's balanced_logloss: 0.307474\n",
      "[1100]\tvalid_0's balanced_logloss: 0.302536\n",
      "[1200]\tvalid_0's balanced_logloss: 0.301495\n",
      "[1300]\tvalid_0's balanced_logloss: 0.297785\n",
      "[1400]\tvalid_0's balanced_logloss: 0.295214\n",
      "[1500]\tvalid_0's balanced_logloss: 0.29376\n",
      "Early stopping, best iteration is:\n",
      "[1459]\tvalid_0's balanced_logloss: 0.292757\n",
      "training model for fold: 4/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.611524\n",
      "[200]\tvalid_0's balanced_logloss: 0.482566\n",
      "[300]\tvalid_0's balanced_logloss: 0.420567\n",
      "[400]\tvalid_0's balanced_logloss: 0.387252\n",
      "[500]\tvalid_0's balanced_logloss: 0.370309\n",
      "[600]\tvalid_0's balanced_logloss: 0.362988\n",
      "[700]\tvalid_0's balanced_logloss: 0.35831\n",
      "[800]\tvalid_0's balanced_logloss: 0.360879\n",
      "Early stopping, best iteration is:\n",
      "[725]\tvalid_0's balanced_logloss: 0.356703\n",
      "training model for fold: 5/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.584271\n",
      "[200]\tvalid_0's balanced_logloss: 0.441279\n",
      "[300]\tvalid_0's balanced_logloss: 0.366763\n",
      "[400]\tvalid_0's balanced_logloss: 0.322317\n",
      "[500]\tvalid_0's balanced_logloss: 0.291121\n",
      "[600]\tvalid_0's balanced_logloss: 0.267209\n",
      "[700]\tvalid_0's balanced_logloss: 0.252435\n",
      "[800]\tvalid_0's balanced_logloss: 0.239772\n",
      "[900]\tvalid_0's balanced_logloss: 0.233535\n",
      "[1000]\tvalid_0's balanced_logloss: 0.224779\n",
      "[1100]\tvalid_0's balanced_logloss: 0.219257\n",
      "[1200]\tvalid_0's balanced_logloss: 0.214974\n",
      "[1300]\tvalid_0's balanced_logloss: 0.211629\n",
      "[1400]\tvalid_0's balanced_logloss: 0.207478\n",
      "[1500]\tvalid_0's balanced_logloss: 0.204877\n",
      "[1600]\tvalid_0's balanced_logloss: 0.202801\n",
      "[1700]\tvalid_0's balanced_logloss: 0.203539\n",
      "Early stopping, best iteration is:\n",
      "[1621]\tvalid_0's balanced_logloss: 0.202356\n",
      "REPEAT NUMBER: 7/10\n",
      "training model for fold: 1/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.678595\n",
      "[200]\tvalid_0's balanced_logloss: 0.57578\n",
      "[300]\tvalid_0's balanced_logloss: 0.523034\n",
      "[400]\tvalid_0's balanced_logloss: 0.493834\n",
      "[500]\tvalid_0's balanced_logloss: 0.47992\n",
      "[600]\tvalid_0's balanced_logloss: 0.475519\n",
      "[700]\tvalid_0's balanced_logloss: 0.472101\n",
      "Early stopping, best iteration is:\n",
      "[665]\tvalid_0's balanced_logloss: 0.470776\n",
      "training model for fold: 2/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.578429\n",
      "[200]\tvalid_0's balanced_logloss: 0.438745\n",
      "[300]\tvalid_0's balanced_logloss: 0.367819\n",
      "[400]\tvalid_0's balanced_logloss: 0.328449\n",
      "[500]\tvalid_0's balanced_logloss: 0.30109\n",
      "[600]\tvalid_0's balanced_logloss: 0.27848\n",
      "[700]\tvalid_0's balanced_logloss: 0.262781\n",
      "[800]\tvalid_0's balanced_logloss: 0.251722\n",
      "[900]\tvalid_0's balanced_logloss: 0.243643\n",
      "[1000]\tvalid_0's balanced_logloss: 0.236116\n",
      "[1100]\tvalid_0's balanced_logloss: 0.22838\n",
      "[1200]\tvalid_0's balanced_logloss: 0.224059\n",
      "[1300]\tvalid_0's balanced_logloss: 0.220625\n",
      "[1400]\tvalid_0's balanced_logloss: 0.218285\n",
      "[1500]\tvalid_0's balanced_logloss: 0.215666\n",
      "[1600]\tvalid_0's balanced_logloss: 0.21368\n",
      "[1700]\tvalid_0's balanced_logloss: 0.209501\n",
      "[1800]\tvalid_0's balanced_logloss: 0.210888\n",
      "Early stopping, best iteration is:\n",
      "[1700]\tvalid_0's balanced_logloss: 0.209501\n",
      "training model for fold: 3/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.602255\n",
      "[200]\tvalid_0's balanced_logloss: 0.460402\n",
      "[300]\tvalid_0's balanced_logloss: 0.379799\n",
      "[400]\tvalid_0's balanced_logloss: 0.324633\n",
      "[500]\tvalid_0's balanced_logloss: 0.293349\n",
      "[600]\tvalid_0's balanced_logloss: 0.266431\n",
      "[700]\tvalid_0's balanced_logloss: 0.246557\n",
      "[800]\tvalid_0's balanced_logloss: 0.233489\n",
      "[900]\tvalid_0's balanced_logloss: 0.222594\n",
      "[1000]\tvalid_0's balanced_logloss: 0.212591\n",
      "[1100]\tvalid_0's balanced_logloss: 0.207441\n",
      "[1200]\tvalid_0's balanced_logloss: 0.204472\n",
      "[1300]\tvalid_0's balanced_logloss: 0.199428\n",
      "[1400]\tvalid_0's balanced_logloss: 0.195972\n",
      "[1500]\tvalid_0's balanced_logloss: 0.1921\n",
      "[1600]\tvalid_0's balanced_logloss: 0.190609\n",
      "[1700]\tvalid_0's balanced_logloss: 0.188285\n",
      "[1800]\tvalid_0's balanced_logloss: 0.185587\n",
      "[1900]\tvalid_0's balanced_logloss: 0.183611\n",
      "[2000]\tvalid_0's balanced_logloss: 0.181951\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1992]\tvalid_0's balanced_logloss: 0.181818\n",
      "training model for fold: 4/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.61818\n",
      "[200]\tvalid_0's balanced_logloss: 0.485994\n",
      "[300]\tvalid_0's balanced_logloss: 0.416498\n",
      "[400]\tvalid_0's balanced_logloss: 0.377411\n",
      "[500]\tvalid_0's balanced_logloss: 0.353896\n",
      "[600]\tvalid_0's balanced_logloss: 0.341328\n",
      "[700]\tvalid_0's balanced_logloss: 0.330757\n",
      "[800]\tvalid_0's balanced_logloss: 0.323828\n",
      "[900]\tvalid_0's balanced_logloss: 0.317514\n",
      "[1000]\tvalid_0's balanced_logloss: 0.312877\n",
      "[1100]\tvalid_0's balanced_logloss: 0.30774\n",
      "[1200]\tvalid_0's balanced_logloss: 0.30624\n",
      "[1300]\tvalid_0's balanced_logloss: 0.304873\n",
      "Early stopping, best iteration is:\n",
      "[1255]\tvalid_0's balanced_logloss: 0.304441\n",
      "training model for fold: 5/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001757 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.596494\n",
      "[200]\tvalid_0's balanced_logloss: 0.461316\n",
      "[300]\tvalid_0's balanced_logloss: 0.390117\n",
      "[400]\tvalid_0's balanced_logloss: 0.33844\n",
      "[500]\tvalid_0's balanced_logloss: 0.30641\n",
      "[600]\tvalid_0's balanced_logloss: 0.279653\n",
      "[700]\tvalid_0's balanced_logloss: 0.257169\n",
      "[800]\tvalid_0's balanced_logloss: 0.238587\n",
      "[900]\tvalid_0's balanced_logloss: 0.22236\n",
      "[1000]\tvalid_0's balanced_logloss: 0.211027\n",
      "[1100]\tvalid_0's balanced_logloss: 0.199869\n",
      "[1200]\tvalid_0's balanced_logloss: 0.19347\n",
      "[1300]\tvalid_0's balanced_logloss: 0.185038\n",
      "[1400]\tvalid_0's balanced_logloss: 0.179576\n",
      "[1500]\tvalid_0's balanced_logloss: 0.173573\n",
      "[1600]\tvalid_0's balanced_logloss: 0.169711\n",
      "[1700]\tvalid_0's balanced_logloss: 0.164275\n",
      "[1800]\tvalid_0's balanced_logloss: 0.161559\n",
      "[1900]\tvalid_0's balanced_logloss: 0.159022\n",
      "[2000]\tvalid_0's balanced_logloss: 0.157191\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1998]\tvalid_0's balanced_logloss: 0.157135\n",
      "REPEAT NUMBER: 8/10\n",
      "training model for fold: 1/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.639415\n",
      "[200]\tvalid_0's balanced_logloss: 0.514713\n",
      "[300]\tvalid_0's balanced_logloss: 0.45075\n",
      "[400]\tvalid_0's balanced_logloss: 0.41106\n",
      "[500]\tvalid_0's balanced_logloss: 0.38482\n",
      "[600]\tvalid_0's balanced_logloss: 0.370284\n",
      "[700]\tvalid_0's balanced_logloss: 0.361479\n",
      "[800]\tvalid_0's balanced_logloss: 0.355716\n",
      "[900]\tvalid_0's balanced_logloss: 0.350525\n",
      "[1000]\tvalid_0's balanced_logloss: 0.34756\n",
      "[1100]\tvalid_0's balanced_logloss: 0.34184\n",
      "[1200]\tvalid_0's balanced_logloss: 0.34392\n",
      "Early stopping, best iteration is:\n",
      "[1110]\tvalid_0's balanced_logloss: 0.341479\n",
      "training model for fold: 2/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.597435\n",
      "[200]\tvalid_0's balanced_logloss: 0.458349\n",
      "[300]\tvalid_0's balanced_logloss: 0.388373\n",
      "[400]\tvalid_0's balanced_logloss: 0.341646\n",
      "[500]\tvalid_0's balanced_logloss: 0.314107\n",
      "[600]\tvalid_0's balanced_logloss: 0.297778\n",
      "[700]\tvalid_0's balanced_logloss: 0.281232\n",
      "[800]\tvalid_0's balanced_logloss: 0.268289\n",
      "[900]\tvalid_0's balanced_logloss: 0.258628\n",
      "[1000]\tvalid_0's balanced_logloss: 0.250601\n",
      "[1100]\tvalid_0's balanced_logloss: 0.244862\n",
      "[1200]\tvalid_0's balanced_logloss: 0.240315\n",
      "[1300]\tvalid_0's balanced_logloss: 0.237571\n",
      "[1400]\tvalid_0's balanced_logloss: 0.234565\n",
      "[1500]\tvalid_0's balanced_logloss: 0.232227\n",
      "[1600]\tvalid_0's balanced_logloss: 0.230168\n",
      "[1700]\tvalid_0's balanced_logloss: 0.229248\n",
      "[1800]\tvalid_0's balanced_logloss: 0.227586\n",
      "[1900]\tvalid_0's balanced_logloss: 0.226852\n",
      "Early stopping, best iteration is:\n",
      "[1897]\tvalid_0's balanced_logloss: 0.226495\n",
      "training model for fold: 3/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.600683\n",
      "[200]\tvalid_0's balanced_logloss: 0.47228\n",
      "[300]\tvalid_0's balanced_logloss: 0.408267\n",
      "[400]\tvalid_0's balanced_logloss: 0.377\n",
      "[500]\tvalid_0's balanced_logloss: 0.357114\n",
      "[600]\tvalid_0's balanced_logloss: 0.345908\n",
      "[700]\tvalid_0's balanced_logloss: 0.337995\n",
      "[800]\tvalid_0's balanced_logloss: 0.334545\n",
      "[900]\tvalid_0's balanced_logloss: 0.331001\n",
      "[1000]\tvalid_0's balanced_logloss: 0.331589\n",
      "[1100]\tvalid_0's balanced_logloss: 0.329101\n",
      "Early stopping, best iteration is:\n",
      "[1056]\tvalid_0's balanced_logloss: 0.328526\n",
      "training model for fold: 4/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001317 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.607563\n",
      "[200]\tvalid_0's balanced_logloss: 0.471205\n",
      "[300]\tvalid_0's balanced_logloss: 0.401154\n",
      "[400]\tvalid_0's balanced_logloss: 0.356457\n",
      "[500]\tvalid_0's balanced_logloss: 0.327315\n",
      "[600]\tvalid_0's balanced_logloss: 0.306073\n",
      "[700]\tvalid_0's balanced_logloss: 0.291537\n",
      "[800]\tvalid_0's balanced_logloss: 0.280822\n",
      "[900]\tvalid_0's balanced_logloss: 0.272157\n",
      "[1000]\tvalid_0's balanced_logloss: 0.266444\n",
      "[1100]\tvalid_0's balanced_logloss: 0.261396\n",
      "[1200]\tvalid_0's balanced_logloss: 0.257147\n",
      "[1300]\tvalid_0's balanced_logloss: 0.252782\n",
      "[1400]\tvalid_0's balanced_logloss: 0.248097\n",
      "Early stopping, best iteration is:\n",
      "[1371]\tvalid_0's balanced_logloss: 0.247508\n",
      "training model for fold: 5/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.590202\n",
      "[200]\tvalid_0's balanced_logloss: 0.440818\n",
      "[300]\tvalid_0's balanced_logloss: 0.362239\n",
      "[400]\tvalid_0's balanced_logloss: 0.31464\n",
      "[500]\tvalid_0's balanced_logloss: 0.284304\n",
      "[600]\tvalid_0's balanced_logloss: 0.261336\n",
      "[700]\tvalid_0's balanced_logloss: 0.246866\n",
      "[800]\tvalid_0's balanced_logloss: 0.2365\n",
      "[900]\tvalid_0's balanced_logloss: 0.222807\n",
      "[1000]\tvalid_0's balanced_logloss: 0.214802\n",
      "[1100]\tvalid_0's balanced_logloss: 0.208303\n",
      "[1200]\tvalid_0's balanced_logloss: 0.203125\n",
      "[1300]\tvalid_0's balanced_logloss: 0.199659\n",
      "[1400]\tvalid_0's balanced_logloss: 0.19544\n",
      "[1500]\tvalid_0's balanced_logloss: 0.193451\n",
      "[1600]\tvalid_0's balanced_logloss: 0.19087\n",
      "[1700]\tvalid_0's balanced_logloss: 0.190778\n",
      "[1800]\tvalid_0's balanced_logloss: 0.189161\n",
      "[1900]\tvalid_0's balanced_logloss: 0.188102\n",
      "[2000]\tvalid_0's balanced_logloss: 0.188855\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1906]\tvalid_0's balanced_logloss: 0.187954\n",
      "REPEAT NUMBER: 9/10\n",
      "training model for fold: 1/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001232 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.576294\n",
      "[200]\tvalid_0's balanced_logloss: 0.431519\n",
      "[300]\tvalid_0's balanced_logloss: 0.359476\n",
      "[400]\tvalid_0's balanced_logloss: 0.318299\n",
      "[500]\tvalid_0's balanced_logloss: 0.292508\n",
      "[600]\tvalid_0's balanced_logloss: 0.277279\n",
      "[700]\tvalid_0's balanced_logloss: 0.264282\n",
      "[800]\tvalid_0's balanced_logloss: 0.255089\n",
      "[900]\tvalid_0's balanced_logloss: 0.246287\n",
      "[1000]\tvalid_0's balanced_logloss: 0.238743\n",
      "[1100]\tvalid_0's balanced_logloss: 0.234221\n",
      "[1200]\tvalid_0's balanced_logloss: 0.230999\n",
      "[1300]\tvalid_0's balanced_logloss: 0.229093\n",
      "[1400]\tvalid_0's balanced_logloss: 0.227481\n",
      "[1500]\tvalid_0's balanced_logloss: 0.225551\n",
      "[1600]\tvalid_0's balanced_logloss: 0.22253\n",
      "[1700]\tvalid_0's balanced_logloss: 0.22072\n",
      "[1800]\tvalid_0's balanced_logloss: 0.219822\n",
      "[1900]\tvalid_0's balanced_logloss: 0.218302\n",
      "[2000]\tvalid_0's balanced_logloss: 0.217512\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1962]\tvalid_0's balanced_logloss: 0.216857\n",
      "training model for fold: 2/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.563027\n",
      "[200]\tvalid_0's balanced_logloss: 0.411319\n",
      "[300]\tvalid_0's balanced_logloss: 0.334792\n",
      "[400]\tvalid_0's balanced_logloss: 0.290363\n",
      "[500]\tvalid_0's balanced_logloss: 0.262015\n",
      "[600]\tvalid_0's balanced_logloss: 0.238097\n",
      "[700]\tvalid_0's balanced_logloss: 0.219902\n",
      "[800]\tvalid_0's balanced_logloss: 0.204788\n",
      "[900]\tvalid_0's balanced_logloss: 0.194088\n",
      "[1000]\tvalid_0's balanced_logloss: 0.184998\n",
      "[1100]\tvalid_0's balanced_logloss: 0.176955\n",
      "[1200]\tvalid_0's balanced_logloss: 0.170141\n",
      "[1300]\tvalid_0's balanced_logloss: 0.164814\n",
      "[1400]\tvalid_0's balanced_logloss: 0.161435\n",
      "[1500]\tvalid_0's balanced_logloss: 0.158326\n",
      "[1600]\tvalid_0's balanced_logloss: 0.156107\n",
      "[1700]\tvalid_0's balanced_logloss: 0.152114\n",
      "[1800]\tvalid_0's balanced_logloss: 0.148903\n",
      "[1900]\tvalid_0's balanced_logloss: 0.14847\n",
      "[2000]\tvalid_0's balanced_logloss: 0.147946\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1927]\tvalid_0's balanced_logloss: 0.147691\n",
      "training model for fold: 3/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.641134\n",
      "[200]\tvalid_0's balanced_logloss: 0.519733\n",
      "[300]\tvalid_0's balanced_logloss: 0.456972\n",
      "[400]\tvalid_0's balanced_logloss: 0.418204\n",
      "[500]\tvalid_0's balanced_logloss: 0.395077\n",
      "[600]\tvalid_0's balanced_logloss: 0.382454\n",
      "[700]\tvalid_0's balanced_logloss: 0.373006\n",
      "[800]\tvalid_0's balanced_logloss: 0.372131\n",
      "[900]\tvalid_0's balanced_logloss: 0.369427\n",
      "[1000]\tvalid_0's balanced_logloss: 0.369412\n",
      "Early stopping, best iteration is:\n",
      "[934]\tvalid_0's balanced_logloss: 0.368282\n",
      "training model for fold: 4/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.620997\n",
      "[200]\tvalid_0's balanced_logloss: 0.487403\n",
      "[300]\tvalid_0's balanced_logloss: 0.41727\n",
      "[400]\tvalid_0's balanced_logloss: 0.376293\n",
      "[500]\tvalid_0's balanced_logloss: 0.348059\n",
      "[600]\tvalid_0's balanced_logloss: 0.330688\n",
      "[700]\tvalid_0's balanced_logloss: 0.316017\n",
      "[800]\tvalid_0's balanced_logloss: 0.308108\n",
      "[900]\tvalid_0's balanced_logloss: 0.301483\n",
      "[1000]\tvalid_0's balanced_logloss: 0.294932\n",
      "[1100]\tvalid_0's balanced_logloss: 0.29165\n",
      "[1200]\tvalid_0's balanced_logloss: 0.288539\n",
      "[1300]\tvalid_0's balanced_logloss: 0.285439\n",
      "[1400]\tvalid_0's balanced_logloss: 0.283681\n",
      "[1500]\tvalid_0's balanced_logloss: 0.283106\n",
      "Early stopping, best iteration is:\n",
      "[1495]\tvalid_0's balanced_logloss: 0.282541\n",
      "training model for fold: 5/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.611621\n",
      "[200]\tvalid_0's balanced_logloss: 0.476776\n",
      "[300]\tvalid_0's balanced_logloss: 0.410256\n",
      "[400]\tvalid_0's balanced_logloss: 0.365368\n",
      "[500]\tvalid_0's balanced_logloss: 0.341288\n",
      "[600]\tvalid_0's balanced_logloss: 0.319269\n",
      "[700]\tvalid_0's balanced_logloss: 0.304544\n",
      "[800]\tvalid_0's balanced_logloss: 0.293081\n",
      "[900]\tvalid_0's balanced_logloss: 0.283004\n",
      "[1000]\tvalid_0's balanced_logloss: 0.276867\n",
      "[1100]\tvalid_0's balanced_logloss: 0.272037\n",
      "[1200]\tvalid_0's balanced_logloss: 0.270455\n",
      "[1300]\tvalid_0's balanced_logloss: 0.265018\n",
      "Early stopping, best iteration is:\n",
      "[1299]\tvalid_0's balanced_logloss: 0.264944\n",
      "REPEAT NUMBER: 10/10\n",
      "training model for fold: 1/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.580823\n",
      "[200]\tvalid_0's balanced_logloss: 0.432511\n",
      "[300]\tvalid_0's balanced_logloss: 0.35572\n",
      "[400]\tvalid_0's balanced_logloss: 0.307752\n",
      "[500]\tvalid_0's balanced_logloss: 0.276891\n",
      "[600]\tvalid_0's balanced_logloss: 0.25376\n",
      "[700]\tvalid_0's balanced_logloss: 0.236639\n",
      "[800]\tvalid_0's balanced_logloss: 0.222546\n",
      "[900]\tvalid_0's balanced_logloss: 0.209683\n",
      "[1000]\tvalid_0's balanced_logloss: 0.20234\n",
      "[1100]\tvalid_0's balanced_logloss: 0.196594\n",
      "[1200]\tvalid_0's balanced_logloss: 0.191893\n",
      "[1300]\tvalid_0's balanced_logloss: 0.187851\n",
      "[1400]\tvalid_0's balanced_logloss: 0.184279\n",
      "[1500]\tvalid_0's balanced_logloss: 0.182\n",
      "[1600]\tvalid_0's balanced_logloss: 0.178529\n",
      "[1700]\tvalid_0's balanced_logloss: 0.176187\n",
      "[1800]\tvalid_0's balanced_logloss: 0.173814\n",
      "[1900]\tvalid_0's balanced_logloss: 0.174343\n",
      "[2000]\tvalid_0's balanced_logloss: 0.173677\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1993]\tvalid_0's balanced_logloss: 0.173263\n",
      "training model for fold: 2/5\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.633367\n",
      "[200]\tvalid_0's balanced_logloss: 0.513492\n",
      "[300]\tvalid_0's balanced_logloss: 0.457138\n",
      "[400]\tvalid_0's balanced_logloss: 0.428175\n",
      "[500]\tvalid_0's balanced_logloss: 0.416609\n",
      "[600]\tvalid_0's balanced_logloss: 0.410515\n",
      "[700]\tvalid_0's balanced_logloss: 0.408914\n",
      "Early stopping, best iteration is:\n",
      "[677]\tvalid_0's balanced_logloss: 0.40698\n",
      "training model for fold: 3/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.611556\n",
      "[200]\tvalid_0's balanced_logloss: 0.477624\n",
      "[300]\tvalid_0's balanced_logloss: 0.408021\n",
      "[400]\tvalid_0's balanced_logloss: 0.362117\n",
      "[500]\tvalid_0's balanced_logloss: 0.338824\n",
      "[600]\tvalid_0's balanced_logloss: 0.323882\n",
      "[700]\tvalid_0's balanced_logloss: 0.313276\n",
      "[800]\tvalid_0's balanced_logloss: 0.30613\n",
      "[900]\tvalid_0's balanced_logloss: 0.300757\n",
      "[1000]\tvalid_0's balanced_logloss: 0.292998\n",
      "[1100]\tvalid_0's balanced_logloss: 0.287098\n",
      "[1200]\tvalid_0's balanced_logloss: 0.28482\n",
      "[1300]\tvalid_0's balanced_logloss: 0.284994\n",
      "[1400]\tvalid_0's balanced_logloss: 0.284226\n",
      "Early stopping, best iteration is:\n",
      "[1363]\tvalid_0's balanced_logloss: 0.283657\n",
      "training model for fold: 4/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.631106\n",
      "[200]\tvalid_0's balanced_logloss: 0.499143\n",
      "[300]\tvalid_0's balanced_logloss: 0.430587\n",
      "[400]\tvalid_0's balanced_logloss: 0.392882\n",
      "[500]\tvalid_0's balanced_logloss: 0.367657\n",
      "[600]\tvalid_0's balanced_logloss: 0.348799\n",
      "[700]\tvalid_0's balanced_logloss: 0.337242\n",
      "[800]\tvalid_0's balanced_logloss: 0.33104\n",
      "[900]\tvalid_0's balanced_logloss: 0.325296\n",
      "[1000]\tvalid_0's balanced_logloss: 0.322904\n",
      "[1100]\tvalid_0's balanced_logloss: 0.320018\n",
      "Early stopping, best iteration is:\n",
      "[1057]\tvalid_0's balanced_logloss: 0.319339\n",
      "training model for fold: 5/5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's balanced_logloss: 0.602852\n",
      "[200]\tvalid_0's balanced_logloss: 0.466645\n",
      "[300]\tvalid_0's balanced_logloss: 0.396839\n",
      "[400]\tvalid_0's balanced_logloss: 0.35566\n",
      "[500]\tvalid_0's balanced_logloss: 0.328825\n",
      "[600]\tvalid_0's balanced_logloss: 0.308052\n",
      "[700]\tvalid_0's balanced_logloss: 0.297514\n",
      "[800]\tvalid_0's balanced_logloss: 0.29356\n",
      "[900]\tvalid_0's balanced_logloss: 0.290182\n",
      "[1000]\tvalid_0's balanced_logloss: 0.286732\n",
      "[1100]\tvalid_0's balanced_logloss: 0.283054\n",
      "[1200]\tvalid_0's balanced_logloss: 0.279391\n",
      "[1300]\tvalid_0's balanced_logloss: 0.279185\n",
      "[1400]\tvalid_0's balanced_logloss: 0.279593\n",
      "[1500]\tvalid_0's balanced_logloss: 0.279451\n",
      "Early stopping, best iteration is:\n",
      "[1448]\tvalid_0's balanced_logloss: 0.278357\n"
     ]
    }
   ],
   "source": [
    "VERBOSE = False\n",
    "\n",
    "models = list()\n",
    "metrics = list()\n",
    "\n",
    "\n",
    "for repeat in range(REPETITIONS):\n",
    "    print(f\"REPEAT NUMBER: {repeat+1}/{REPETITIONS}\")\n",
    "    cv_split = repeated_cv_split[f\"repeat_{repeat}\"]\n",
    "    n_folds = len(cv_split)\n",
    "    \n",
    "    for split in cv_split:\n",
    "        fold = split[\"fold\"]\n",
    "        train_idx = split[\"train_idx\"]\n",
    "        valid_idx = split[\"valid_idx\"]\n",
    "        print(f\"training model for fold: {fold+1}/{n_folds}\")\n",
    "    \n",
    "        train_df = train.loc[train_idx,:].reset_index(drop=True)\n",
    "        valid_df = train.loc[valid_idx,:].reset_index(drop=True)\n",
    "\n",
    "        train_dset = lgb.Dataset(\n",
    "            data=train_df.loc[:,input_cols],\n",
    "            label=train_df.loc[:,\"Class\"].values,\n",
    "            free_raw_data=False\n",
    "        )\n",
    "        valid_dset = lgb.Dataset(\n",
    "            data=valid_df.loc[:,input_cols],\n",
    "            label=valid_df.loc[:,\"Class\"].values,\n",
    "            free_raw_data=False\n",
    "        )\n",
    "        model = lgb.train(\n",
    "            params=model_params,\n",
    "            train_set=train_dset,\n",
    "            valid_sets=[valid_dset,],\n",
    "            num_boost_round=2000,\n",
    "            feval=[balanced_logloss],\n",
    "            callbacks=[lgb.log_evaluation(period=100), lgb.early_stopping(100)],\n",
    "        )\n",
    "        \n",
    "        if VERBOSE:\n",
    "            lgb.plot_importance(model, importance_type=\"gain\", figsize=(8,15))\n",
    "            plt.show()\n",
    "            lgb.plot_importance(model, importance_type=\"split\", figsize=(8,15))\n",
    "            plt.show()\n",
    "\n",
    "        models.append(model)\n",
    "        metrics.append(model.best_score[\"valid_0\"][\"balanced_logloss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9155172c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T16:44:32.064746Z",
     "iopub.status.busy": "2023-06-15T16:44:32.064356Z",
     "iopub.status.idle": "2023-06-15T16:44:32.411745Z",
     "shell.execute_reply": "2023-06-15T16:44:32.410474Z"
    },
    "papermill": {
     "duration": 0.424884,
     "end_time": "2023-06-15T16:44:32.414341",
     "exception": false,
     "start_time": "2023-06-15T16:44:31.989457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_metrics = metrics[:50]\n",
    "\n",
    "metric_mean = np.mean(_metrics)\n",
    "metric_median = np.median(_metrics)\n",
    "metric_std = np.std(_metrics)\n",
    "\n",
    "plt.hist(_metrics, bins=10)\n",
    "plt.axvline(metric_mean, c=\"r\")\n",
    "plt.title(f\"mean={metric_mean:0.4f}  &  std={metric_std:0.4f}\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca2766f",
   "metadata": {
    "papermill": {
     "duration": 0.073274,
     "end_time": "2023-06-15T16:44:32.561500",
     "exception": false,
     "start_time": "2023-06-15T16:44:32.488226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***\n",
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6305ca09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T16:44:32.709669Z",
     "iopub.status.busy": "2023-06-15T16:44:32.709201Z",
     "iopub.status.idle": "2023-06-15T16:44:32.833506Z",
     "shell.execute_reply": "2023-06-15T16:44:32.832419Z"
    },
    "papermill": {
     "duration": 0.2012,
     "end_time": "2023-06-15T16:44:32.836070",
     "exception": false,
     "start_time": "2023-06-15T16:44:32.634870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10959411, 0.10959411, 0.10959411, 0.10959411, 0.10959411])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probs = [model.predict(test[input_cols]) for model in models]\n",
    "predicted_probs = np.mean(predicted_probs, axis=0)\n",
    "predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd080ada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T16:44:32.987657Z",
     "iopub.status.busy": "2023-06-15T16:44:32.987237Z",
     "iopub.status.idle": "2023-06-15T16:44:33.013444Z",
     "shell.execute_reply": "2023-06-15T16:44:33.012226Z"
    },
    "papermill": {
     "duration": 0.104912,
     "end_time": "2023-06-15T16:44:33.016093",
     "exception": false,
     "start_time": "2023-06-15T16:44:32.911181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.904059e-01</td>\n",
       "      <td>0.109594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.241267e-16</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.904059e-01</td>\n",
       "      <td>0.109594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.904059e-01</td>\n",
       "      <td>0.109594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.904059e-01</td>\n",
       "      <td>0.109594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.904059e-01</td>\n",
       "      <td>0.109594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.904059e-01</td>\n",
       "      <td>0.109594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            class_0   class_1\n",
       "count  5.000000e+00  5.000000\n",
       "mean   8.904059e-01  0.109594\n",
       "std    1.241267e-16  0.000000\n",
       "min    8.904059e-01  0.109594\n",
       "25%    8.904059e-01  0.109594\n",
       "50%    8.904059e-01  0.109594\n",
       "75%    8.904059e-01  0.109594\n",
       "max    8.904059e-01  0.109594"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv\")\n",
    "sub[\"class_0\"] = 1-predicted_probs\n",
    "sub[\"class_1\"] = predicted_probs\n",
    "sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32431ad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T16:44:33.168711Z",
     "iopub.status.busy": "2023-06-15T16:44:33.168304Z",
     "iopub.status.idle": "2023-06-15T16:44:33.180059Z",
     "shell.execute_reply": "2023-06-15T16:44:33.178722Z"
    },
    "papermill": {
     "duration": 0.092093,
     "end_time": "2023-06-15T16:44:33.183053",
     "exception": false,
     "start_time": "2023-06-15T16:44:33.090960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6e7fd",
   "metadata": {
    "papermill": {
     "duration": 0.0756,
     "end_time": "2023-06-15T16:44:33.334089",
     "exception": false,
     "start_time": "2023-06-15T16:44:33.258489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 283.735312,
   "end_time": "2023-06-15T16:44:34.633717",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-15T16:39:50.898405",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
